{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN8oEz63WlkwsPtwHEq5Qdl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"j5tR1FtJ4K2V","executionInfo":{"status":"ok","timestamp":1706945338978,"user_tz":-540,"elapsed":6151,"user":{"displayName":"고대화","userId":"04859921805535355794"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import gym\n","import numpy as np\n","import random\n","from collections import deque\n","\n","# Factorized Gaussian Noise Layer\n","class NoisyLinear(nn.Module):\n","    def __init__(self, in_features, out_features, sigma_init=0.5, bias=True):\n","        super(NoisyLinear, self).__init__()\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.sigma_init = sigma_init\n","\n","        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n","        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n","        self.register_buffer('weight_epsilon', torch.FloatTensor(out_features, in_features))\n","\n","        if bias:\n","            self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n","            self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n","            self.register_buffer('bias_epsilon', torch.FloatTensor(out_features))\n","        else:\n","            self.register_parameter('bias_mu', None)\n","            self.register_parameter('bias_sigma', None)\n","\n","        self.reset_parameters()\n","        self.reset_noise()\n","\n","    def reset_parameters(self):\n","        mu_range = 1 / np.sqrt(self.in_features)\n","        self.weight_mu.data.uniform_(-mu_range, mu_range)\n","        self.weight_sigma.data.fill_(self.sigma_init / np.sqrt(self.in_features))\n","\n","        if self.bias_mu is not None:\n","            self.bias_mu.data.uniform_(-mu_range, mu_range)\n","            self.bias_sigma.data.fill_(self.sigma_init / np.sqrt(self.out_features))\n","\n","    def reset_noise(self):\n","        epsilon_in = self._scale_noise(self.in_features)\n","        epsilon_out = self._scale_noise(self.out_features)\n","\n","        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n","        if self.bias_mu is not None:\n","            self.bias_epsilon.copy_(epsilon_out)\n","\n","    def _scale_noise(self, size):\n","        x = torch.randn(size)\n","        return x.sign().mul_(x.abs().sqrt_())\n","\n","    def forward(self, input):\n","        if self.training:\n","            return F.linear(input, self.weight_mu + self.weight_sigma * self.weight_epsilon,\n","                            self.bias_mu + self.bias_sigma * self.bias_epsilon)\n","        else:\n","            return F.linear(input, self.weight_mu, self.bias_mu)\n","\n","# Neural Network Model Defined at Here.\n","class Network(nn.Module):\n","    def __init__(self, state_size, action_size, hidden_size):\n","        super(Network, self).__init__()\n","        self.fc1 = NoisyLinear(state_size, hidden_size)\n","        self.fc2 = NoisyLinear(hidden_size, hidden_size)\n","        self.fc3 = NoisyLinear(hidden_size, action_size)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.fc3(x)\n","\n","class DQNAgent:\n","    def __init__(self, env, batch_size, target_update, hidden_size):\n","        self.env = env\n","        self.state_size = env.observation_space.shape[0]\n","        self.action_size = env.action_space.n\n","        self.batch_size = batch_size\n","        self.target_update = target_update\n","        self.gamma = 0.99\n","\n","        self.dqn = Network(self.state_size, self.action_size, hidden_size)\n","        self.dqn_target = Network(self.state_size, self.action_size, hidden_size)\n","        self.optimizer = optim.Adam(self.dqn.parameters(), lr=0.001)\n","\n","        self.memory = deque(maxlen=10000)\n","        self._target_hard_update()\n","\n","    def get_action(self, state, epsilon):\n","        if random.random() > epsilon:\n","            with torch.no_grad():\n","                state = torch.tensor([state], dtype=torch.float32)\n","                q_value = self.dqn(state)\n","                action = q_value.max(1)[1].item()\n","        else:\n","            action = self.env.action_space.sample()\n","        return action\n","\n","    def append_sample(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def train_step(self):\n","        if len(self.memory) < self.batch_size:\n","            return\n","\n","        mini_batch = random.sample(self.memory, self.batch_size)\n","        states, actions, rewards, next_states, dones = zip(*mini_batch)\n","\n","        states = torch.tensor(states, dtype=torch.float32)\n","        actions = torch.tensor(actions, dtype=torch.int64)\n","        rewards = torch.tensor(rewards, dtype=torch.float32)\n","        next_states = torch.tensor(next_states, dtype=torch.float32)\n","        dones = torch.tensor(dones, dtype=torch.float32)\n","\n","        curr_Qs = self.dqn(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n","        next_Qs = self.dqn_target(next_states).max(1)[0]\n","        expected_Qs = rewards + self.gamma * next_Qs * (1 - dones)\n","\n","        loss = F.mse_loss(curr_Qs, expected_Qs.detach())\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def _target_hard_update(self):\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n"]},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","# Main training loop\n","env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","\n","hidden_size = 128\n","max_episodes = 500\n","batch_size = 64\n","target_update = 100\n","\n","epsilon = 1.0\n","max_epsilon = 1.0\n","min_epsilon = 0.01\n","decay_rate = 0.005\n","\n","agent = DQNAgent(env, batch_size, target_update, hidden_size)\n","\n","scores = []\n","\n","with tqdm(total=max_episodes, desc=\"에피소드 진행\") as pbar:\n","    for episode in range(max_episodes):\n","        state = env.reset()\n","        episode_reward = 0\n","        done = False\n","\n","        while not done:\n","            action = agent.get_action(state, epsilon)\n","            next_state, reward, done, _ = env.step(action)\n","            agent.append_sample(state, action, reward, next_state, done)\n","\n","            state = next_state\n","            episode_reward += reward\n","\n","            agent.train_step()\n","\n","            if episode % agent.target_update == 0:\n","                agent._target_hard_update()\n","\n","        scores.append(episode_reward)\n","        pbar.update(1)\n","        pbar.set_postfix({'episode_reward': episode_reward})\n","        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n","\n","print(f\"평균 점수: {sum(scores) / max_episodes}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"akXHbsrt4dCp","outputId":"4a5c0bc4-0df1-4225-9baa-f3468e7d4313"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","에피소드 진행:   1%|          | 3/500 [00:00<00:05, 97.02it/s, episode_reward=31]<ipython-input-1-f5952a4908ce>:110: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n","  states = torch.tensor(states, dtype=torch.float32)\n","에피소드 진행:  74%|███████▎  | 368/500 [00:50<00:54,  2.40it/s, episode_reward=71]"]}]},{"cell_type":"code","source":[],"metadata":{"id":"wpfSwjiF470P"},"execution_count":null,"outputs":[]}]}