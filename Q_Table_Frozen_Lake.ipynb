{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyONlwSBG+GkHfyV/mHoOFpU"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 1. IMPORTING LIBRARIES"],"metadata":{"id":"kdn7AdZi5n_P"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"FAy0_1SS5CHP","executionInfo":{"status":"ok","timestamp":1699422310745,"user_tz":-540,"elapsed":6646,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"277c9a8f-519f-497d-83ad-6062cebb5494"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym[toy_text] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[toy_text]) (1.23.5)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[toy_text]) (2.2.1)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[toy_text]) (0.0.8)\n","Requirement already satisfied: pygame==2.1.0 in /usr/local/lib/python3.10/dist-packages (from gym[toy_text]) (2.1.0)\n"]}],"source":["import sys\n","import random\n","import gym\n","import numpy as np\n","from IPython.display import clear_output\n","\n","from tqdm import tqdm\n","import time\n","\n","! pip install gym[toy_text]"]},{"cell_type":"markdown","source":["# 2. CREATING THE ENVIRONMENT\n","새로운 환경을 생성합니다."],"metadata":{"id":"y0JOC6Ul5sjk"}},{"cell_type":"code","source":["env_name = \"FrozenLake-v1\"\n","env = gym.make(env_name)\n","env.seed(777)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jg4eDgET5net","executionInfo":{"status":"ok","timestamp":1699422313533,"user_tz":-540,"elapsed":3,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"a36d1c86-d158-4736-e94e-5093e707debe"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]},{"output_type":"execute_result","data":{"text/plain":["[777]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["class Agent:\n","    def __init__(\n","        self,\n","        env: gym.Env,\n","    ):\n","        \"\"\" 초기화\n","\n","        Args:\n","            env (gym.Env): openAI Gym 환경\n","            epsilon_decay (float): epsilon을 감소시키기 위한 스텝 크기\n","            lr (float): 학습률\n","            max_epsilon (float): epsilon의 최대값\n","            min_epsilon (float): epsilon의 최소값\n","            gamma (float): discount factor\n","        \"\"\"\n","\n","        # QTable 생성\n","        self.env = env\n","\n","        self.state_size  = self.env.observation_space.n\n","        self.action_size = self.env.action_space.n\n","\n","        self.lr = 0.9\n","        self.gamma = 0.99\n","\n","        self.qtable = np.zeros((self.state_size, self.action_size))\n","\n","    # EXPLORATION VS EXPLOITATION\n","    def get_action(self, state, epsilon):\n","        q_value = self.qtable[state,:]\n","        # 3. 현재 월드 상태에서 액션 a를 선택합니다. (s)\n","        # 만약 이 숫자가 엡실론보다 작으면 무작위 선택을 하게 됨 --> exploration\n","        if np.random.rand() <= epsilon:\n","            action = np.random.choice(self.action_size)\n","\n","        ## 그렇지 않다면 --> exploitation (해당 상태에 대해 가장 큰 Q 값을 선택)\n","        else:\n","            action = np.argmax(q_value)\n","\n","        return action\n","\n","    def train_step(self, state, action, reward, next_state, done):\n","\n","        curr_Q = self.qtable[state, action]\n","        next_Q = self.qtable[next_state, :]\n","\n","        # Update Q(s,a):= Q(s,a) + lr [R(s,a) + gamma * max Q(s',a) - Q(s,a)]\n","        self.qtable[state, action] = curr_Q + self.lr * (reward + gamma * np.max(next_Q) - curr_Q)"],"metadata":{"id":"prpedxp45xtK","executionInfo":{"status":"ok","timestamp":1699422318395,"user_tz":-540,"elapsed":1,"user":{"displayName":"고대화","userId":"04859921805535355794"}}},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":["# 4. INITIALIZING THE Q-PARAMETERS"],"metadata":{"id":"kNHb5SJF6-XZ"}},{"cell_type":"code","source":["max_episodes = 5000  # 에이전트를 훈련시킬 전체 에피소드 수 설정.\n","\n","max_steps = 99       # 에피소드 당 최대 스텝 수\n","gamma = 0.95         # 할인율\n","render = False       # 게임 환경을 표시할지 여부\n","\n","# 탐험 매개변수\n","epsilon = 1.0        # 탐험률\n","max_epsilon = 1.0    # 시작 시 탐험 확률\n","min_epsilon = 0.01   # 최소 탐험 확률\n","decay_rate = 0.005   # 탐험 확률의 지수적 감소율"],"metadata":{"id":"h6kl2I5F6bXN","executionInfo":{"status":"ok","timestamp":1699422320812,"user_tz":-540,"elapsed":1,"user":{"displayName":"고대화","userId":"04859921805535355794"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["# 5. TRAINING LOOP"],"metadata":{"id":"ZKvmJgPp4JeK"}},{"cell_type":"code","source":["# train\n","agent = Agent(\n","    env,\n","#     memory_size,\n","#     batch_size,\n","#     epsilon_decay,\n",")\n","\n","# 에이전트에게 제공된 모든 에피소드의 보상을 담을 리스트\n","scores = []\n","\n","# 진행률 표시줄 설정\n","progress_bar = tqdm(range(max_episodes), desc=\"Episode Progress\", unit=\"episode\")\n","\n","# 각 에피소드를 위한 반복문\n","for episode in progress_bar:\n","    state = agent.env.reset()\n","    episode_reward = 0\n","    done = False\n","\n","    # 에피소드가 끝날 때까지 반복\n","    while not done:\n","        action = agent.get_action(state, epsilon)\n","        next_state, reward, done, _ = agent.env.step(action)\n","        agent.train_step(state, action, reward, next_state, done)\n","\n","        state = next_state\n","        episode_reward += reward\n","\n","        if done:\n","            scores.append(episode_reward)\n","            progress_bar.set_postfix(episode_reward=episode_reward)\n","\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n","\n","# 시간에 따른 평균 점수 출력\n","print (\"Score over time: \" +  str(sum(scores)/max_episodes))\n","print(agent.qtable) # Q-테이블 출력\n","\n","# 천 개 에피소드당 평균 보상 계산 및 출력\n","count = 1000\n","rewards_per_thousand_episodes = np.split(np.array(scores),int(max_episodes/count))\n","\n","print(\"\\n\\n********1000 episode마다 평균 리워드********\\n\")\n","for r in rewards_per_thousand_episodes:\n","    print(count, \": \", str(sum(r/1000)))\n","    count += 1000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0izg889A5z28","executionInfo":{"status":"ok","timestamp":1699422460434,"user_tz":-540,"elapsed":23379,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"44d60f6c-e58e-4780-fb58-6f187e79644d"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["Episode Progress: 100%|██████████| 5000/5000 [00:23<00:00, 215.76episode/s, episode_reward=1]\n"]},{"output_type":"stream","name":"stdout","text":["Score over time: 0.4112\n","[[3.69833783e-01 8.44168698e-02 5.65451225e-02 5.03575066e-02]\n"," [1.28788837e-03 1.14971828e-02 3.72820566e-03 4.38927229e-01]\n"," [2.92946452e-04 5.97829975e-03 1.26822667e-02 7.68191603e-02]\n"," [1.39443487e-05 2.16498650e-04 1.35338091e-03 2.25292421e-02]\n"," [2.02159411e-01 3.35962185e-02 1.55697516e-05 3.05334979e-02]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [2.51793744e-09 1.43499861e-08 8.01912499e-02 9.42251876e-07]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [3.08853048e-03 4.44968880e-02 1.02096697e-03 4.30687143e-01]\n"," [8.09889246e-04 6.02156572e-01 9.92926852e-03 1.40180983e-03]\n"," [3.90321346e-01 1.35641306e-03 9.89845767e-04 3.18962850e-06]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [5.27095536e-02 6.80427351e-02 8.15706495e-01 6.62143979e-02]\n"," [3.37597079e-02 9.85459354e-01 9.62575651e-02 1.87196920e-02]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n","\n","\n","********1000 episode마다 평균 리워드********\n","\n","1000 :  0.18400000000000014\n","2000 :  0.47600000000000037\n","3000 :  0.4850000000000004\n","4000 :  0.46500000000000036\n","5000 :  0.44600000000000034\n"]}]},{"cell_type":"markdown","source":["# 6.TEST\n","Q-테이블에 따라 최선의 행동을 선택하여 Frozen Lake 게임을 하는 에이전트 관찰해봅시다."],"metadata":{"id":"wGLA33FrdlCz"}},{"cell_type":"code","source":["for episode in range(5):\n","    state = agent.env.reset()\n","    step = 0\n","    done = False\n","    print(\"****************************************************\")\n","    print(\"*******Episode \", episode+1, \"*******\\n\\n\\n\\n\")\n","    time.sleep(1)\n","\n","    for step in range(max_steps):\n","        # 현재 환경 상태를 화면에 표시\n","        clear_output(wait=True)\n","        # 현재 상태에 대해 가장 높은 Q-값을 가지는 행동 선택\n","        print(env.render(mode='ansi'))\n","        time.sleep(0.3)\n","\n","        # 현재 상태에 대해 예상되는 최대 미래 보상을 가진 행동(인덱스)을 취함\n","        action = np.argmax(agent.qtable[state,:])\n","\n","        new_state, reward, done, info = agent.env.step(action)\n","\n","        if done:\n","            # 여기서는 에이전트가 목표에 도착했는지, 구멍에 빠졌는지 보기 위해 마지막 상태만 출력합니다.\n","            if new_state == 15:\n","                print(\"잘 도착했습니다 🏆\")\n","            else:\n","                print(\"구덩이에 빠졌네요 ☠️\")\n","\n","            # 몇 단계가 걸렸는지 출력합니다.\n","            print(\"Number of steps\", step)\n","\n","            break\n","        state = new_state\n","agent.env.close()"],"metadata":{"id":"4hB_kV_66T7A","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1699422580214,"user_tz":-540,"elapsed":64318,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"693f862a-8f21-4208-cf3f-ae6f533dc104"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["  (Right)\n","SFFF\n","FHFH\n","FFFH\n","HF\u001b[41mF\u001b[0mG\n","\n","잘 도착했습니다 🏆\n","Number of steps 50\n"]}]}]}