{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzOSuYabmjHUayOLuLUTCb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":94,"metadata":{"id":"BUZMlfhKaox9","executionInfo":{"status":"ok","timestamp":1703910187636,"user_tz":-540,"elapsed":895846,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"f13e477e-ff2c-4b99-ae7a-a89758e96974","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["Episode: 1/200, Score: 63.0, Epsilon: 1.0000\n","Episode: 2/200, Score: 18.0, Epsilon: 1.0000\n","Episode: 3/200, Score: 11.0, Epsilon: 0.9756\n","Episode: 4/200, Score: 40.0, Epsilon: 0.9517\n","Episode: 5/200, Score: 11.0, Epsilon: 0.9285\n","Episode: 6/200, Score: 22.0, Epsilon: 0.9058\n","Episode: 7/200, Score: 11.0, Epsilon: 0.8837\n","Episode: 8/200, Score: 13.0, Epsilon: 0.8621\n","Episode: 9/200, Score: 16.0, Epsilon: 0.8411\n","Episode: 10/200, Score: 34.0, Epsilon: 0.8205\n","Episode: 11/200, Score: 14.0, Epsilon: 0.8005\n","Episode: 12/200, Score: 27.0, Epsilon: 0.7810\n","Episode: 13/200, Score: 15.0, Epsilon: 0.7620\n","Episode: 14/200, Score: 16.0, Epsilon: 0.7434\n","Episode: 15/200, Score: 17.0, Epsilon: 0.7253\n","Episode: 16/200, Score: 16.0, Epsilon: 0.7076\n","Episode: 17/200, Score: 17.0, Epsilon: 0.6904\n","Episode: 18/200, Score: 18.0, Epsilon: 0.6736\n","Episode: 19/200, Score: 13.0, Epsilon: 0.6572\n","Episode: 20/200, Score: 22.0, Epsilon: 0.6413\n","Episode: 21/200, Score: 10.0, Epsilon: 0.6257\n","Episode: 22/200, Score: 9.0, Epsilon: 0.6105\n","Episode: 23/200, Score: 8.0, Epsilon: 0.5956\n","Episode: 24/200, Score: 18.0, Epsilon: 0.5812\n","Episode: 25/200, Score: 10.0, Epsilon: 0.5671\n","Episode: 26/200, Score: 11.0, Epsilon: 0.5533\n","Episode: 27/200, Score: 12.0, Epsilon: 0.5399\n","Episode: 28/200, Score: 12.0, Epsilon: 0.5268\n","Episode: 29/200, Score: 11.0, Epsilon: 0.5141\n","Episode: 30/200, Score: 10.0, Epsilon: 0.5016\n","Episode: 31/200, Score: 13.0, Epsilon: 0.4895\n","Episode: 32/200, Score: 8.0, Epsilon: 0.4776\n","Episode: 33/200, Score: 11.0, Epsilon: 0.4661\n","Episode: 34/200, Score: 10.0, Epsilon: 0.4548\n","Episode: 35/200, Score: 24.0, Epsilon: 0.4439\n","Episode: 36/200, Score: 10.0, Epsilon: 0.4331\n","Episode: 37/200, Score: 18.0, Epsilon: 0.4227\n","Episode: 38/200, Score: 12.0, Epsilon: 0.4125\n","Episode: 39/200, Score: 19.0, Epsilon: 0.4026\n","Episode: 40/200, Score: 10.0, Epsilon: 0.3929\n","Episode: 41/200, Score: 18.0, Epsilon: 0.3834\n","Episode: 42/200, Score: 13.0, Epsilon: 0.3742\n","Episode: 43/200, Score: 11.0, Epsilon: 0.3652\n","Episode: 44/200, Score: 8.0, Epsilon: 0.3564\n","Episode: 45/200, Score: 10.0, Epsilon: 0.3479\n","Episode: 46/200, Score: 19.0, Epsilon: 0.3395\n","Episode: 47/200, Score: 11.0, Epsilon: 0.3314\n","Episode: 48/200, Score: 11.0, Epsilon: 0.3235\n","Episode: 49/200, Score: 14.0, Epsilon: 0.3157\n","Episode: 50/200, Score: 11.0, Epsilon: 0.3082\n","Episode: 51/200, Score: 10.0, Epsilon: 0.3008\n","Episode: 52/200, Score: 9.0, Epsilon: 0.2936\n","Episode: 53/200, Score: 10.0, Epsilon: 0.2866\n","Episode: 54/200, Score: 8.0, Epsilon: 0.2798\n","Episode: 55/200, Score: 10.0, Epsilon: 0.2731\n","Episode: 56/200, Score: 10.0, Epsilon: 0.2666\n","Episode: 57/200, Score: 8.0, Epsilon: 0.2603\n","Episode: 58/200, Score: 12.0, Epsilon: 0.2541\n","Episode: 59/200, Score: 13.0, Epsilon: 0.2481\n","Episode: 60/200, Score: 11.0, Epsilon: 0.2422\n","Episode: 61/200, Score: 15.0, Epsilon: 0.2365\n","Episode: 62/200, Score: 12.0, Epsilon: 0.2309\n","Episode: 63/200, Score: 11.0, Epsilon: 0.2254\n","Episode: 64/200, Score: 11.0, Epsilon: 0.2201\n","Episode: 65/200, Score: 8.0, Epsilon: 0.2149\n","Episode: 66/200, Score: 11.0, Epsilon: 0.2099\n","Episode: 67/200, Score: 10.0, Epsilon: 0.2049\n","Episode: 68/200, Score: 9.0, Epsilon: 0.2001\n","Episode: 69/200, Score: 11.0, Epsilon: 0.1954\n","Episode: 70/200, Score: 11.0, Epsilon: 0.1909\n","Episode: 71/200, Score: 9.0, Epsilon: 0.1864\n","Episode: 72/200, Score: 12.0, Epsilon: 0.1820\n","Episode: 73/200, Score: 10.0, Epsilon: 0.1778\n","Episode: 74/200, Score: 10.0, Epsilon: 0.1736\n","Episode: 75/200, Score: 11.0, Epsilon: 0.1696\n","Episode: 76/200, Score: 9.0, Epsilon: 0.1657\n","Episode: 77/200, Score: 10.0, Epsilon: 0.1618\n","Episode: 78/200, Score: 10.0, Epsilon: 0.1581\n","Episode: 79/200, Score: 9.0, Epsilon: 0.1544\n","Episode: 80/200, Score: 10.0, Epsilon: 0.1509\n","Episode: 81/200, Score: 12.0, Epsilon: 0.1474\n","Episode: 82/200, Score: 9.0, Epsilon: 0.1440\n","Episode: 83/200, Score: 12.0, Epsilon: 0.1407\n","Episode: 84/200, Score: 10.0, Epsilon: 0.1374\n","Episode: 85/200, Score: 11.0, Epsilon: 0.1343\n","Episode: 86/200, Score: 10.0, Epsilon: 0.1312\n","Episode: 87/200, Score: 10.0, Epsilon: 0.1282\n","Episode: 88/200, Score: 11.0, Epsilon: 0.1253\n","Episode: 89/200, Score: 10.0, Epsilon: 0.1225\n","Episode: 90/200, Score: 10.0, Epsilon: 0.1197\n","Episode: 91/200, Score: 10.0, Epsilon: 0.1170\n","Episode: 92/200, Score: 9.0, Epsilon: 0.1143\n","Episode: 93/200, Score: 10.0, Epsilon: 0.1118\n","Episode: 94/200, Score: 8.0, Epsilon: 0.1093\n","Episode: 95/200, Score: 9.0, Epsilon: 0.1068\n","Episode: 96/200, Score: 10.0, Epsilon: 0.1044\n","Episode: 97/200, Score: 9.0, Epsilon: 0.1021\n","Episode: 98/200, Score: 9.0, Epsilon: 0.0998\n","Episode: 99/200, Score: 8.0, Epsilon: 0.0976\n","Episode: 100/200, Score: 10.0, Epsilon: 0.0954\n","Episode: 101/200, Score: 8.0, Epsilon: 0.0933\n","Episode: 102/200, Score: 9.0, Epsilon: 0.0913\n","Episode: 103/200, Score: 10.0, Epsilon: 0.0893\n","Episode: 104/200, Score: 10.0, Epsilon: 0.0873\n","Episode: 105/200, Score: 11.0, Epsilon: 0.0854\n","Episode: 106/200, Score: 12.0, Epsilon: 0.0835\n","Episode: 107/200, Score: 10.0, Epsilon: 0.0817\n","Episode: 108/200, Score: 11.0, Epsilon: 0.0799\n","Episode: 109/200, Score: 9.0, Epsilon: 0.0782\n","Episode: 110/200, Score: 10.0, Epsilon: 0.0765\n","Episode: 111/200, Score: 9.0, Epsilon: 0.0749\n","Episode: 112/200, Score: 9.0, Epsilon: 0.0733\n","Episode: 113/200, Score: 11.0, Epsilon: 0.0717\n","Episode: 114/200, Score: 11.0, Epsilon: 0.0702\n","Episode: 115/200, Score: 10.0, Epsilon: 0.0687\n","Episode: 116/200, Score: 10.0, Epsilon: 0.0673\n","Episode: 117/200, Score: 8.0, Epsilon: 0.0659\n","Episode: 118/200, Score: 11.0, Epsilon: 0.0645\n","Episode: 119/200, Score: 9.0, Epsilon: 0.0631\n","Episode: 120/200, Score: 11.0, Epsilon: 0.0618\n","Episode: 121/200, Score: 11.0, Epsilon: 0.0605\n","Episode: 122/200, Score: 9.0, Epsilon: 0.0593\n","Episode: 123/200, Score: 9.0, Epsilon: 0.0581\n","Episode: 124/200, Score: 9.0, Epsilon: 0.0569\n","Episode: 125/200, Score: 9.0, Epsilon: 0.0557\n","Episode: 126/200, Score: 9.0, Epsilon: 0.0546\n","Episode: 127/200, Score: 10.0, Epsilon: 0.0535\n","Episode: 128/200, Score: 9.0, Epsilon: 0.0524\n","Episode: 129/200, Score: 9.0, Epsilon: 0.0514\n","Episode: 130/200, Score: 12.0, Epsilon: 0.0504\n","Episode: 131/200, Score: 10.0, Epsilon: 0.0494\n","Episode: 132/200, Score: 9.0, Epsilon: 0.0484\n","Episode: 133/200, Score: 10.0, Epsilon: 0.0474\n","Episode: 134/200, Score: 10.0, Epsilon: 0.0465\n","Episode: 135/200, Score: 10.0, Epsilon: 0.0456\n","Episode: 136/200, Score: 9.0, Epsilon: 0.0447\n","Episode: 137/200, Score: 8.0, Epsilon: 0.0439\n","Episode: 138/200, Score: 11.0, Epsilon: 0.0430\n","Episode: 139/200, Score: 10.0, Epsilon: 0.0422\n","Episode: 140/200, Score: 10.0, Epsilon: 0.0414\n","Episode: 141/200, Score: 8.0, Epsilon: 0.0407\n","Episode: 142/200, Score: 10.0, Epsilon: 0.0399\n","Episode: 143/200, Score: 9.0, Epsilon: 0.0392\n","Episode: 144/200, Score: 10.0, Epsilon: 0.0384\n","Episode: 145/200, Score: 10.0, Epsilon: 0.0377\n","Episode: 146/200, Score: 10.0, Epsilon: 0.0371\n","Episode: 147/200, Score: 9.0, Epsilon: 0.0364\n","Episode: 148/200, Score: 8.0, Epsilon: 0.0357\n","Episode: 149/200, Score: 12.0, Epsilon: 0.0351\n","Episode: 150/200, Score: 9.0, Epsilon: 0.0345\n","Episode: 151/200, Score: 9.0, Epsilon: 0.0339\n","Episode: 152/200, Score: 14.0, Epsilon: 0.0333\n","Episode: 153/200, Score: 9.0, Epsilon: 0.0327\n","Episode: 154/200, Score: 11.0, Epsilon: 0.0321\n","Episode: 155/200, Score: 10.0, Epsilon: 0.0316\n","Episode: 156/200, Score: 8.0, Epsilon: 0.0311\n","Episode: 157/200, Score: 9.0, Epsilon: 0.0305\n","Episode: 158/200, Score: 9.0, Epsilon: 0.0300\n","Episode: 159/200, Score: 9.0, Epsilon: 0.0295\n","Episode: 160/200, Score: 9.0, Epsilon: 0.0291\n","Episode: 161/200, Score: 10.0, Epsilon: 0.0286\n","Episode: 162/200, Score: 9.0, Epsilon: 0.0281\n","Episode: 163/200, Score: 10.0, Epsilon: 0.0277\n","Episode: 164/200, Score: 10.0, Epsilon: 0.0272\n","Episode: 165/200, Score: 11.0, Epsilon: 0.0268\n","Episode: 166/200, Score: 9.0, Epsilon: 0.0264\n","Episode: 167/200, Score: 9.0, Epsilon: 0.0260\n","Episode: 168/200, Score: 9.0, Epsilon: 0.0256\n","Episode: 169/200, Score: 12.0, Epsilon: 0.0252\n","Episode: 170/200, Score: 10.0, Epsilon: 0.0248\n","Episode: 171/200, Score: 9.0, Epsilon: 0.0245\n","Episode: 172/200, Score: 8.0, Epsilon: 0.0241\n","Episode: 173/200, Score: 8.0, Epsilon: 0.0238\n","Episode: 174/200, Score: 9.0, Epsilon: 0.0234\n","Episode: 175/200, Score: 10.0, Epsilon: 0.0231\n","Episode: 176/200, Score: 9.0, Epsilon: 0.0228\n","Episode: 177/200, Score: 11.0, Epsilon: 0.0225\n","Episode: 178/200, Score: 10.0, Epsilon: 0.0222\n","Episode: 179/200, Score: 9.0, Epsilon: 0.0219\n","Episode: 180/200, Score: 10.0, Epsilon: 0.0216\n","Episode: 181/200, Score: 8.0, Epsilon: 0.0213\n","Episode: 182/200, Score: 10.0, Epsilon: 0.0210\n","Episode: 183/200, Score: 10.0, Epsilon: 0.0207\n","Episode: 184/200, Score: 10.0, Epsilon: 0.0205\n","Episode: 185/200, Score: 10.0, Epsilon: 0.0202\n","Episode: 186/200, Score: 9.0, Epsilon: 0.0200\n","Episode: 187/200, Score: 8.0, Epsilon: 0.0197\n","Episode: 188/200, Score: 10.0, Epsilon: 0.0195\n","Episode: 189/200, Score: 9.0, Epsilon: 0.0192\n","Episode: 190/200, Score: 9.0, Epsilon: 0.0190\n","Episode: 191/200, Score: 10.0, Epsilon: 0.0188\n","Episode: 192/200, Score: 8.0, Epsilon: 0.0186\n","Episode: 193/200, Score: 10.0, Epsilon: 0.0184\n","Episode: 194/200, Score: 9.0, Epsilon: 0.0181\n","Episode: 195/200, Score: 8.0, Epsilon: 0.0179\n","Episode: 196/200, Score: 10.0, Epsilon: 0.0178\n","Episode: 197/200, Score: 11.0, Epsilon: 0.0176\n","Episode: 198/200, Score: 10.0, Epsilon: 0.0174\n","Episode: 199/200, Score: 10.0, Epsilon: 0.0172\n","Episode: 200/200, Score: 8.0, Epsilon: 0.0170\n"]}],"source":["import random\n","import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from collections import deque\n","from torch.utils.data import DataLoader, Dataset\n","\n","class ReplayBuffer(Dataset):\n","    def __init__(self, capacity=10000):\n","        self.memory = deque(maxlen=capacity)\n","\n","    def put(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def __len__(self):\n","        return len(self.memory)\n","\n","    def __getitem__(self, idx):\n","        return self.memory[idx]\n","\n","class Network(nn.Module):\n","    def __init__(self, state_size, action_size, atoms, z):\n","        super(Network, self).__init__()\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.atoms = atoms\n","        self.z = z\n","\n","        self.layers = nn.Sequential(\n","            nn.Linear(self.state_size, 64),\n","            nn.ReLU(),\n","            nn.Linear(64, 64),\n","            nn.ReLU()\n","        )\n","        self.value_streams = nn.ModuleList([nn.Linear(64, self.atoms) for _ in range(self.action_size)])\n","\n","    def forward(self, x):\n","        x = self.layers(x)\n","        return [torch.softmax(value_stream(x), dim=-1) for value_stream in self.value_streams]\n","\n","class DQNAgent:\n","    def __init__(self, env, batch_size, target_update, v_max, v_min, atoms):\n","        self.env = env\n","        self.env.seed(1234)\n","        self.state_size = env.observation_space.shape[0]\n","        self.action_size = env.action_space.n\n","\n","        self.batch_size = batch_size\n","        self.v_max = v_max\n","        self.v_min = v_min\n","        self.atoms = atoms\n","        self.delta_z = float(self.v_max - self.v_min) / (self.atoms - 1)\n","        self.z = torch.linspace(self.v_min, self.v_max, steps=self.atoms)\n","\n","        self.dqn = Network(self.state_size, self.action_size, self.atoms, self.z)\n","        self.dqn_target = Network(self.state_size, self.action_size, self.atoms, self.z)\n","        self.optimizer = optim.Adam(self.dqn.parameters(), lr=0.001)\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.memory = ReplayBuffer(capacity=10000)\n","        self.target_update = target_update\n","        self.gamma = 0.99\n","        self.Soft_Update = False\n","        self.TAU = 0.1\n","        self.train_start = 1000\n","        self._target_hard_update()\n","\n","    def get_action(self, state, epsilon):\n","        state = torch.FloatTensor(state).unsqueeze(0)\n","        if random.random() <= epsilon:\n","            action = random.randrange(self.action_size)\n","        else:\n","            with torch.no_grad():\n","                z = self.dqn(state)\n","                z_concat = torch.cat(z, dim=1).view(-1, self.action_size, self.atoms)\n","                z_broadcast = self.z.view(1, 1, self.atoms)\n","                q = torch.sum(z_concat * z_broadcast, dim=2)\n","                action = q.argmax(1).item()\n","        return action\n","\n","    def train_step(self, states, actions, rewards, next_states, dones):\n","        states = torch.FloatTensor(states)\n","        actions = torch.LongTensor(actions)\n","        rewards = torch.FloatTensor(rewards)\n","        next_states = torch.FloatTensor(next_states)\n","        dones = torch.FloatTensor(dones)\n","\n","        with torch.no_grad():\n","            z = self.dqn(next_states)\n","            z_ = self.dqn_target(next_states)\n","            z_concat = np.vstack(z)\n","            z_broadcast = self.z.view(1, 1, self.atoms)\n","            q = np.sum(np.multiply(z_concat, np.array(self.z)), axis=1)\n","            q = q.reshape((self.batch_size, self.action_size), order='F')\n","            next_actions = np.argmax(q, axis=1)\n","            m_prob = [np.zeros((self.batch_size, self.atoms))\n","                      for _ in range(self.action_size)]\n","            for i in range(self.batch_size):\n","                if dones[i]:\n","                    T_zj = min(self.v_max, max(self.v_min, rewards[i]))\n","                    b_j  = (T_zj - self.v_min) / self.delta_z\n","                    l, u = math.floor(b_j), math.ceil(b_j)\n","                    m_prob[actions[i]][i][int(l)] += (u - b_j)\n","                    m_prob[actions[i]][i][int(u)] += (b_j - l)\n","                else:\n","                    for j in range(self.atoms):\n","                        T_zj = min(self.v_max, max(\n","                            self.v_min, rewards[i] + self.gamma * self.z[j]))\n","                        b_j = (T_zj - self.v_min) / self.delta_z\n","                        l, u = math.floor(b_j), math.ceil(b_j)\n","                        m_prob[actions[i]][i][int(\n","                            l)] += z_[next_actions[i]][i][j] * (u - b_j)\n","                        m_prob[actions[i]][i][int(\n","                            u)] += z_[next_actions[i]][i][j] * (b_j - l)\n","\n","        main_value = self.dqn(states)[0]\n","        m_prob = torch.tensor(m_prob[0])\n","        loss = self.criterion(m_prob, main_value)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","\n","    def _target_hard_update(self):\n","        if not self.Soft_Update:\n","            self.dqn_target.load_state_dict(self.dqn.state_dict())\n","        else:\n","            for target_param, param in zip(self.dqn_target.parameters(), self.dqn.parameters()):\n","                target_param.data.copy_(target_param.data * (1.0 - self.TAU) + param.data * self.TAU)\n","\n","    def save(self, name):\n","        torch.save(self.dqn.state_dict(), name)\n","\n","    def load(self, name):\n","        self.dqn.load_state_dict(torch.load(name))\n","        self.dqn_target.load_state_dict(torch.load(name))\n","\n","# Main Training Loop\n","env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","\n","# parameters\n","hidden_size = 128\n","max_episodes = 200\n","batch_size = 64\n","\n","# Exploration parameters\n","epsilon = 1.0\n","max_epsilon = 1.0\n","min_epsilon = 0.01\n","decay_rate = 0.025\n","\n","atoms = 51\n","v_min = -10.\n","v_max = 10.\n","\n","agent = DQNAgent(env, batch_size, 20, v_max, v_min, atoms)\n","\n","for episode in range(max_episodes):\n","    state = env.reset()\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        action = agent.get_action(state, epsilon)\n","        next_state, reward, done, _ = env.step(action)\n","        agent.memory.put(state, action, reward, next_state, done)\n","\n","        state = next_state\n","        episode_reward += reward\n","\n","        if done:\n","            print(f\"Episode: {episode + 1}/{max_episodes}, Score: {episode_reward}, Epsilon: {epsilon:.4f}\")\n","            break\n","\n","        if len(agent.memory) >= batch_size:\n","            sample = random.sample(agent.memory.memory, batch_size)\n","            states, actions, rewards, next_states, dones = map(np.array, zip(*sample))\n","            agent.train_step(states, actions, rewards, next_states, dones)\n","\n","            if episode % agent.target_update == 0:\n","                agent._target_hard_update()\n","\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n"]},{"cell_type":"code","source":["# IMPORTING LIBRARIES\n","\n","import sys\n","IN_COLAB = \"google.colab\" in sys.modules\n","\n","import random\n","import gym\n","import numpy as np\n","\n","import tensorflow as tf\n","from tensorflow.keras.layers import Input, Dense, Reshape, Softmax\n","from tensorflow.keras import optimizers, losses\n","from tensorflow.keras import Model\n","from collections import deque\n","\n","from IPython.display import clear_output\n","import math\n","\n","tf.keras.backend.set_floatx('float64')\n","\n","class ReplayBuffer:\n","    def __init__(self, capacity=10000):\n","        self.memory = deque(maxlen=capacity)\n","\n","    def put(self, state, action, reward, next_state, done):\n","        self.memory.append([state, action, reward, next_state, done])\n","\n","    def sample(self):\n","        sample = random.sample(self.memory, batch_size)\n","        states, actions, rewards, next_states, done = map(\n","            np.asarray, zip(*sample))\n","        states = np.array(states).reshape(batch_size, -1)\n","        next_states = np.array(next_states).reshape(batch_size, -1)\n","        return states, actions, rewards, next_states, done\n","\n","    def size(self):\n","        return len(self.memory)\n","\n","# Neural Network Model Defined at Here.\n","class Network(Model):\n","    def __init__(self, state_size: int, action_size: int,\n","    z):\n","        \"\"\"Initialization.\"\"\"\n","        super(Network, self).__init__()\n","\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.atoms = atoms\n","        self.z = z\n","\n","        # self.opt = tf.keras.optimizers.Adam(lr)\n","        self.model = self.create_model()\n","\n","    def create_model(self):\n","        input_state = Input((self.state_size,))\n","        h1 = Dense(64, activation='relu')(input_state)\n","        h2 = Dense(64, activation='relu')(h1)\n","        outputs = []\n","        for _ in range(self.action_size):\n","            outputs.append(Dense(self.atoms, activation='softmax')(h2))\n","        return tf.keras.Model(input_state, outputs)\n","\n","    def predict(self, state):\n","        return self.model.predict(state)\n","\n","class DQNAgent:\n","    def __init__(\n","        self,\n","        env: gym.Env,\n","        batch_size: int,\n","        target_update: int,\n","    ):\n","        \"\"\"Initialization.\n","\n","        Args:\n","            env (gym.Env): openAI Gym environment\n","            memory_size (int): length of memory\n","            batch_size (int): batch size for sampling\n","            target_update (int): period for target model's hard update\n","            epsilon_decay (float): step size to decrease epsilon\n","            lr (float): learning rate\n","            max_epsilon (float): max value of epsilon\n","            min_epsilon (float): min value of epsilon\n","            gamma (float): discount factor\n","        \"\"\"\n","\n","        # CREATING THE Q-Network\n","        self.env = env\n","        self.env.seed(1234)\n","        self.state_size = self.env.observation_space.shape[0]\n","        self.action_size = self.env.action_space.n\n","\n","        self.batch_size = batch_size\n","        # hyper parameters\n","        memory_size = 10000\n","        self.v_max = v_max\n","        self.v_min = v_min\n","        self.atoms = atoms\n","        self.delta_z = float(self.v_max - self.v_min) / (self.atoms - 1)\n","        self.z = [self.v_min + i * self.delta_z for i in range(self.atoms)]\n","        self.lr = 0.001\n","        self.target_update = target_update\n","        self.gamma = 0.99    # discount rate\n","\n","        # create main model and target model\n","        self.dqn = Network(self.state_size, self.action_size, self.z\n","                          )\n","        self.dqn_target = Network( self.state_size, self.action_size, self.z\n","                          )\n","        self.train_start = 1000\n","\n","        self.optimizers = optimizers.Adam(lr=self.lr, )\n","        self.criterion = tf.keras.losses.CategoricalCrossentropy()\n","\n","        self.memory = ReplayBuffer(capacity=memory_size)\n","        self.Soft_Update = False # use soft parameter update\n","\n","        self.TAU = 0.1 # target network soft update hyperparameter\n","\n","        self._target_hard_update()\n","\n","    # EXPLORATION VS EXPLOITATION\n","    def get_action(self, state, epsilon):\n","        state = np.reshape(state, [1, self.state_size])\n","        # 3. Choose an action a in the current world state (s)\n","        # If this number < greater than epsilon doing a random choice --> exploration\n","        if np.random.rand() <= epsilon:\n","            action = np.random.choice(self.action_size)\n","\n","        ## Else --> exploitation (taking the biggest Q value for this state)\n","        else:\n","            action = self.get_optimal_action(state)\n","\n","        return action\n","\n","    def get_optimal_action(self, state):\n","        z = self.dqn.predict(state)\n","        z_concat = np.vstack(z)\n","        q = np.sum(np.multiply(z_concat, np.array(self.z)), axis=1)\n","        return np.argmax(q)\n","\n","    # UPDATING THE Q-VALUE\n","    def train_step(self):\n","        states, actions, rewards, next_states, dones = self.memory.sample()\n","        z = self.dqn.predict(next_states)\n","        z_ = self.dqn_target.predict(next_states)\n","        z_concat = np.vstack(z)\n","        print(z_concat.shape)\n","        q = np.sum(np.multiply(z_concat, np.array(self.z)), axis=1)\n","        q = q.reshape((self.batch_size, self.action_size), order='F')\n","        next_actions = np.argmax(q, axis=1)\n","        m_prob = [np.zeros((self.batch_size, self.atoms))\n","                  for _ in range(self.action_size)]\n","        for i in range(self.batch_size):\n","            if dones[i]:\n","                T_zj = min(self.v_max, max(self.v_min, rewards[i]))\n","                b_j  = (T_zj - self.v_min) / self.delta_z\n","                l, u = math.floor(b_j), math.ceil(b_j)\n","                m_prob[actions[i]][i][int(l)] += (u - b_j)\n","                m_prob[actions[i]][i][int(u)] += (b_j - l)\n","            else:\n","                for j in range(self.atoms):\n","                    T_zj = min(self.v_max, max(\n","                        self.v_min, rewards[i] + self.gamma * self.z[j]))\n","                    b_j = (T_zj - self.v_min) / self.delta_z\n","                    l, u = math.floor(b_j), math.ceil(b_j)\n","                    m_prob[actions[i]][i][int(\n","                        l)] += z_[next_actions[i]][i][j] * (u - b_j)\n","                    m_prob[actions[i]][i][int(\n","                        u)] += z_[next_actions[i]][i][j] * (b_j - l)\n","        # self.dqn.train(states, m_prob)\n","        # def train(self, states, targets):\n","        m_prob = tf.stop_gradient(m_prob)\n","        dqn_variable = self.dqn.model.trainable_variables\n","        with tf.GradientTape() as tape:\n","            main_value = self.dqn.model(states)\n","            loss = self.criterion(m_prob, main_value)\n","\n","        dqn_grads = tape.gradient(loss, dqn_variable)\n","        self.optimizers.apply_gradients(zip(dqn_grads, dqn_variable))\n","\n","    # after some time interval update the target model to be same with model\n","    def _target_hard_update(self):\n","        if not self.Soft_Update:\n","            weights = self.dqn.model.get_weights()\n","            self.dqn_target.model.set_weights(weights)\n","            return\n","        if self.Soft_Update:\n","            q_model_theta = self.dqn.model.get_weights()\n","            dqn_target_theta = self.dqn_target.model.get_weights()\n","            counter = 0\n","            for q_weight, target_weight in zip(q_model_theta, dqn_target_theta):\n","                target_weight = target_weight * (1-self.TAU) + q_weight * self.TAU\n","                dqn_target_theta[counter] = target_weight\n","                counter += 1\n","            self.dqn_target.set_weights(dqn_target_theta)\n","\n","    def load(self, name):\n","        self.dqn = load_model(name)\n","\n","    def save(self, name):\n","        self.dqn.save(name)\n","\n","# CREATING THE ENVIRONMENT\n","env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","\n","# parameters\n","target_update = 20\n","\n","\n","# INITIALIZING THE Q-PARAMETERS\n","hidden_size = 128\n","max_episodes = 200  # Set total number of episodes to train agent on.\n","batch_size = 64\n","\n","# Exploration parameters\n","epsilon = 1.0                 # Exploration rate\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability\n","decay_rate = 0.025            # Exponential decay rate for exploration prob\n","\n","atoms = 51\n","v_min = -10.\n","v_max = 10.\n","\n","# train\n","agent = DQNAgent(\n","    env,\n","#     memory_size,\n","    batch_size,\n","    target_update,\n","#     epsilon_decay,\n",")\n","\n","if __name__ == \"__main__\":\n","\n","    update_cnt    = 0\n","    # TRAINING LOOP\n","    #List to contain all the rewards of all the episodes given to the agent\n","    scores = []\n","\n","    # EACH EPISODE\n","    for episode in range(max_episodes):\n","        ## Reset environment and get first new observation\n","        state = agent.env.reset()\n","        episode_reward = 0\n","        done = False  # has the enviroment finished?\n","\n","\n","        # EACH TIME STEP\n","        while not done:\n","        # for step in range(max_steps):  # step index, maximum step is 200\n","\n","            # 3.4.1 EXPLORATION VS EXPLOITATION\n","            # Take the action (a) and observe the outcome state(s') and reward (r)\n","            action = agent.get_action(state, epsilon)\n","\n","            # 2.7.2 TAKING ACTION\n","            next_state, reward, done, _ = agent.env.step(action)\n","            agent.memory.put(state, action, reward, next_state, done)\n","\n","            # Our new state is state\n","            state = next_state\n","\n","            episode_reward += reward\n","\n","            # if episode ends\n","            if done:\n","                scores.append(episode_reward)\n","                print(\"Episode: {}/{}, Score: {}, Epsilon: {:.4}\".format(episode+1, max_episodes, episode_reward, epsilon))\n","                break\n","            # if training is ready\n","            if agent.memory.size() >= agent.batch_size:\n","                # 3.4.2 UPDATING THE Q-VALUE\n","                agent.train_step()\n","                update_cnt += 1\n","\n","                # if hard update is needed\n","                if update_cnt % agent.target_update == 0:\n","                    agent._target_hard_update()\n","\n","        # 2.8 EXPLORATION RATE DECAY\n","        # Reduce epsilon (because we need less and less exploration)\n","        epsilon = min_epsilon + (max_epsilon - min_epsilon)*np.exp(-decay_rate*episode)\n","\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"IRtzra0evsHp","executionInfo":{"status":"error","timestamp":1703908800088,"user_tz":-540,"elapsed":8588,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"79a25ed7-a9ac-4313-c5fa-86647a718fa2"},"execution_count":79,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n","/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"]},{"output_type":"stream","name":"stdout","text":["Episode: 1/200, Score: 16.0, Epsilon: 1.0\n","Episode: 2/200, Score: 10.0, Epsilon: 1.0\n","Episode: 3/200, Score: 19.0, Epsilon: 0.9756\n","2/2 [==============================] - 0s 7ms/step\n","2/2 [==============================] - 0s 6ms/step\n","(128, 51)\n","2/2 [==============================] - 0s 6ms/step\n","2/2 [==============================] - 0s 4ms/step\n","(128, 51)\n","2/2 [==============================] - 0s 6ms/step\n","2/2 [==============================] - 0s 6ms/step\n","(128, 51)\n","2/2 [==============================] - 0s 6ms/step\n","2/2 [==============================] - 0s 4ms/step\n","(128, 51)\n","Episode: 4/200, Score: 23.0, Epsilon: 0.9517\n","2/2 [==============================] - 0s 5ms/step\n","2/2 [==============================] - 0s 6ms/step\n","(128, 51)\n","2/2 [==============================] - 0s 6ms/step\n","2/2 [==============================] - 0s 7ms/step\n","(128, 51)\n","2/2 [==============================] - 0s 5ms/step\n","2/2 [==============================] - 0s 5ms/step\n","(128, 51)\n","2/2 [==============================] - 0s 6ms/step\n","2/2 [==============================] - 0s 6ms/step\n","(128, 51)\n","1/1 [==============================] - 0s 100ms/step\n","2/2 [==============================] - 0s 5ms/step\n","2/2 [==============================] - 0s 9ms/step\n","(128, 51)\n","2/2 [==============================] - 0s 9ms/step\n","2/2 [==============================] - 0s 9ms/step\n","(128, 51)\n","2/2 [==============================] - 0s 9ms/step\n","2/2 [==============================] - 0s 9ms/step\n","(128, 51)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-79-157d83c77f56>\u001b[0m in \u001b[0;36m<cell line: 236>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# 3.4.2 UPDATING THE Q-VALUE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m                 \u001b[0mupdate_cnt\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-79-157d83c77f56>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mz_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdqn_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mz_concat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-79-157d83c77f56>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDQNAgent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   2649\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_steps_per_execution_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m             \u001b[0mbatch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2651\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2652\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1339\u001b[0m         \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1340\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1341\u001b[0;31m             \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1342\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \"not be specified.\")\n\u001b[0;32m--> 706\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    743\u001b[0m             self._flat_output_types)\n\u001b[1;32m    744\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_set_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfulltype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 745\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    746\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    747\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3419\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3420\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3421\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   3422\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   3423\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":[],"metadata":{"id":"5fk1tjim4M3m"},"execution_count":null,"outputs":[]}]}