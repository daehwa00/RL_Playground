{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPXkNmYT1G4yNn7wEXaud/o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"h5NrVxEMH1aN","colab":{"base_uri":"https://localhost:8080/"},"outputId":"71bac13b-0505-4ff5-d59f-1c157f93bef1"},"outputs":[{"output_type":"stream","name":"stdout","text":["--2024-02-03 07:26:52--  https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4283 (4.2K) [text/plain]\n","Saving to: ‘segment_tree.py’\n","\n","\rsegment_tree.py       0%[                    ]       0  --.-KB/s               \rsegment_tree.py     100%[===================>]   4.18K  --.-KB/s    in 0s      \n","\n","2024-02-03 07:26:52 (52.0 MB/s) - ‘segment_tree.py’ saved [4283/4283]\n","\n","cpu\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"output_type":"stream","name":"stdout","text":["Episode 1: 23.0\n","Episode 2: 34.0\n","Episode 3: 14.0\n","Episode 4: 21.0\n","Episode 5: 19.0\n","Episode 6: 18.0\n","Episode 7: 23.0\n","Episode 8: 13.0\n","Episode 9: 10.0\n","Episode 10: 23.0\n","Episode 11: 29.0\n","Episode 12: 13.0\n","Episode 13: 30.0\n","Episode 14: 94.0\n","Episode 15: 29.0\n","Episode 16: 39.0\n","Episode 17: 28.0\n","Episode 18: 14.0\n","Episode 19: 70.0\n","Episode 20: 42.0\n","Episode 21: 42.0\n","Episode 22: 27.0\n","Episode 23: 23.0\n","Episode 24: 17.0\n","Episode 25: 15.0\n","Episode 26: 32.0\n","Episode 27: 53.0\n","Episode 28: 200.0\n","Episode 29: 37.0\n","Episode 30: 35.0\n","Episode 31: 107.0\n","Episode 32: 85.0\n","Episode 33: 27.0\n","Episode 34: 200.0\n","Episode 35: 200.0\n","Episode 36: 200.0\n","Episode 37: 200.0\n","Episode 38: 153.0\n","Episode 39: 143.0\n","Episode 40: 158.0\n","Episode 41: 127.0\n","Episode 42: 149.0\n","Episode 43: 158.0\n","Episode 44: 150.0\n","Episode 45: 154.0\n","Episode 46: 152.0\n","Episode 47: 175.0\n","Episode 48: 149.0\n","Episode 49: 150.0\n","Episode 50: 152.0\n","Episode 51: 149.0\n","Episode 52: 142.0\n","Episode 53: 133.0\n","Episode 54: 138.0\n","Episode 55: 161.0\n","Episode 56: 156.0\n","Episode 57: 137.0\n","Episode 58: 173.0\n","Episode 59: 159.0\n","Episode 60: 157.0\n","Episode 61: 200.0\n","Episode 62: 177.0\n","Episode 63: 200.0\n","Episode 64: 158.0\n","Episode 65: 200.0\n","Episode 66: 191.0\n","Episode 67: 200.0\n","Episode 68: 186.0\n","Episode 69: 200.0\n","Episode 70: 200.0\n","Episode 71: 200.0\n","Episode 72: 200.0\n","Episode 73: 196.0\n","Episode 74: 84.0\n","Episode 75: 200.0\n","Episode 76: 200.0\n","Episode 77: 200.0\n","Episode 78: 200.0\n","Episode 79: 200.0\n","Episode 80: 200.0\n","Episode 81: 200.0\n","Episode 82: 200.0\n"]}],"source":["import sys\n","IN_COLAB = \"google.colab\" in sys.modules\n","\n","import math\n","import os\n","from collections import deque\n","import random\n","from typing import Deque, Dict, List, Tuple\n","\n","import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","from torch.nn.utils import clip_grad_norm_\n","\n","if IN_COLAB and not os.path.exists(\"segment_tree.py\"):\n","    # download segment tree module\n","    !wget https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n","\n","from segment_tree import MinSegmentTree, SumSegmentTree\n","\n","if torch.backends.cudnn.enabled:\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","seed = 777\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","class ReplayBuffer:\n","    \"\"\"A numpy replay buffer.\"\"\"\n","\n","    def __init__(\n","        self,\n","        state_size: int,\n","        size: int,\n","        batch_size: int = 32,\n","        n_step: int = 3,\n","        gamma: float = 0.99,\n","    ):\n","        \"\"\"Initialize.\"\"\"\n","        self.state_memory = np.zeros([size, state_size], dtype=np.float32)\n","        self.action_memory = np.zeros([size], dtype=np.float32)\n","        self.reward_memory = np.zeros([size], dtype=np.float32)\n","        self.next_state_memory = np.zeros([size, state_size], dtype=np.float32)\n","        self.done_memory = np.zeros([size], dtype=np.float32)\n","        self.max_size, self.batch_size = size, batch_size\n","        self.ptr, self.size = 0, 0\n","\n","        # for N-step Learning\n","        self.n_step_buffer = deque(maxlen=n_step)\n","        self.n_step = n_step\n","        self.gamma = gamma\n","\n","    def store(\n","        self,\n","        obs: np.ndarray,\n","        act: np.ndarray,\n","        rew: float,\n","        next_obs: np.ndarray,\n","        done: bool,\n","    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n","        \"\"\"Store the transition in buffer.\"\"\"\n","        transition = (obs, act, rew, next_obs, done)\n","        self.n_step_buffer.append(transition)\n","\n","        # single step transition is not ready\n","        if len(self.n_step_buffer) < self.n_step:\n","            return ()\n","\n","        # make a n-step transition\n","        rew, next_obs, done = self._get_n_step_info(\n","            self.n_step_buffer, self.gamma\n","        )\n","        obs, act = self.n_step_buffer[0][:2]\n","\n","        self.state_memory[self.ptr] = obs\n","        self.action_memory[self.ptr] = act\n","        self.reward_memory[self.ptr] = rew\n","        self.next_state_memory[self.ptr] = next_obs\n","        self.done_memory[self.ptr] = done\n","\n","        self.ptr = (self.ptr + 1) % self.max_size\n","        self.size = min(self.size + 1, self.max_size)\n","\n","        return self.n_step_buffer[0]\n","\n","    def sample_batch(self) -> Dict[str, np.ndarray]:\n","        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n","        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n","\n","        return dict(\n","                    obs=self.state_memory[idxs],\n","                    next_obs=self.next_state_memory[idxs],\n","                    acts=self.action_memory[idxs],\n","                    rews=self.reward_memory[idxs],\n","                    done=self.done_memory[idxs],\n","                    # for N-step Learning\n","                    indices=idxs,\n","                    )\n","\n","    def sample_batch_from_idxs(self, idxs: np.ndarray ) -> Dict[str, np.ndarray]:\n","        # for N-step Learning\n","        return dict(\n","            obs=self.state_memory[idxs],\n","            next_obs=self.next_state_memory[idxs],\n","            acts=self.action_memory[idxs],\n","            rews=self.reward_memory[idxs],\n","            done=self.done_memory[idxs],\n","            )\n","\n","    def _get_n_step_info(\n","        self, n_step_buffer: Deque, gamma: float\n","    ) -> Tuple[np.int64, np.ndarray, bool]:\n","        \"\"\"Return n step rew, next_obs, and done.\"\"\"\n","        # info of the last transition\n","        rew, next_obs, done = self.n_step_buffer[-1][-3:]\n","\n","        for transition in reversed(list(self.n_step_buffer)[:-1]):\n","            r, n_o, d = transition[-3:]\n","\n","            rew = r + self.gamma * rew * (1 - d)\n","            next_obs, done = (n_o, d) if d else (next_obs, done)\n","\n","        return rew, next_obs, done\n","\n","    def __len__(self) -> int:\n","        return self.size\n","\n","# PrioritizedReplayBuffer\n","\n","class PrioritizedReplayBuffer(ReplayBuffer):\n","    \"\"\"Prioritized Replay buffer.\n","\n","    Attributes:\n","        max_priority (float): max priority\n","        tree_ptr (int): next index of tree\n","        alpha (float): alpha parameter for prioritized replay buffer\n","        sum_tree (SumSegmentTree): sum tree for prior\n","\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        state_size: int,\n","        size: int,\n","        batch_size: int = 32,\n","        alpha: float = 0.6,\n","        n_step: int = 1,\n","        gamma: float = 0.99,\n","    ):\n","        \"\"\"Initialization.\"\"\"\n","        assert alpha >= 0\n","\n","        super(PrioritizedReplayBuffer, self).__init__(\n","            state_size, size, batch_size,\n","            n_step, gamma,\n","        )\n","        self.max_priority, self.tree_ptr = 1.0, 0\n","        self.alpha = alpha\n","\n","        # capacity must be positive and a power of 2.\n","        tree_capacity = 1\n","        while tree_capacity < self.max_size:\n","            tree_capacity *= 2\n","\n","        self.sum_tree = SumSegmentTree(tree_capacity)\n","\n","    def store(\n","        self,\n","        obs: np.ndarray,\n","        act: int,\n","        rew: float,\n","        next_obs: np.ndarray,\n","        done: bool,\n","    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:\n","        \"\"\"Store experience and priority.\"\"\"\n","        transition = super().store(obs, act, rew, next_obs, done)\n","\n","        if transition:\n","            self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha\n","\n","            self.tree_ptr = (self.tree_ptr + 1) % self.max_size\n","\n","        return transition\n","\n","    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:\n","        \"\"\"Sample a batch of experiences.\"\"\"\n","        assert len(self) >= self.batch_size\n","        assert beta > 0\n","\n","        indices = self._sample_proportional()\n","\n","        obs = self.state_memory[indices]\n","        next_obs = self.next_state_memory[indices]\n","        acts = self.action_memory[indices]\n","        rews = self.reward_memory[indices]\n","        done = self.done_memory[indices]\n","        weights = np.array([self._calculate_weight(i, beta) for i in indices])\n","\n","        return dict(\n","            obs=obs,\n","            next_obs=next_obs,\n","            acts=acts,\n","            rews=rews,\n","            done=done,\n","            weights=weights,\n","            indices=indices,\n","        )\n","\n","    def update_priorities(self, indices: List[int], priorities: np.ndarray):\n","        \"\"\"Update priorities of sampled transitions.\"\"\"\n","        assert len(indices) == len(priorities)\n","\n","        for idx, priority in zip(indices, priorities):\n","            assert priority > 0\n","            assert 0 <= idx < len(self)\n","\n","            self.sum_tree[idx] = priority ** self.alpha\n","\n","            self.max_priority = max(self.max_priority, priority)\n","\n","    def _sample_proportional(self) -> List[int]:\n","        \"\"\"Sample indices based on proportions.\"\"\"\n","        indices = []\n","        p_total = self.sum_tree.sum(0, len(self) - 1)\n","        segment = p_total / self.batch_size\n","\n","        for i in range(self.batch_size):\n","            a = segment * i\n","            b = segment * (i + 1)\n","            upperbound = random.uniform(a, b)\n","            idx = self.sum_tree.retrieve(upperbound)\n","            indices.append(idx)\n","\n","        return indices\n","\n","    def _calculate_weight(self, idx: int, beta: float):\n","        \"\"\"Calculate the weight of the experience at idx.\"\"\"\n","        # calculate weights\n","        p_sample = self.sum_tree[idx] / self.sum_tree.sum()\n","        weight = (p_sample * len(self)) ** (-beta)\n","\n","        return weight\n","\n","class Network(nn.Module):\n","    def __init__(self, state_size: int, action_size: int,\n","    ):\n","        \"\"\"Initialization.\"\"\"\n","        super(Network, self).__init__()\n","\n","        self.layers = nn.Sequential(\n","            nn.Linear(state_size, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, action_size)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward method implementation.\"\"\"\n","        return self.layers(x)\n","\n","class DQNAgent:\n","    \"\"\"DQN Agent interacting with environment.\n","\n","    Attribute:\n","        env (gym.Env): openAI Gym environment\n","        memory (PrioritizedReplayBuffer): replay memory to store transitions\n","        batch_size (int): batch size for sampling\n","        epsilon (float): parameter for epsilon greedy policy\n","        epsilon_decay (float): step size to decrease epsilon\n","        max_epsilon (float): max value of epsilon\n","        min_epsilon (float): min value of epsilon\n","        target_update (int): period for target model's hard update\n","        gamma (float): discount factor\n","        dqn (Network): model to train and select actions\n","        dqn_target (Network): target model to update\n","        optimizer (torch.optim): optimizer for training dqn\n","        transition (list): transition information including\n","                           state, action, reward, next_state, done\n","        beta (float): determines how much importance sampling is used\n","        prior_eps (float): guarantees every transition can be sampled\n","        use_n_step (bool): whether to use n_step memory\n","        n_step (int): step number to calculate n-step td error\n","        memory_n (ReplayBuffer): n-step replay buffer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        env: gym.Env,\n","        memory_size: int,\n","        batch_size: int,\n","        target_update: int,\n","        epsilon_decay: float,\n","        max_epsilon: float = 1.0,\n","        min_epsilon: float = 0.1,\n","        gamma: float = 0.99,\n","        # PER parameters\n","        alpha: float = 0.2,\n","        beta: float = 0.6,\n","        prior_eps: float = 1e-6,\n","        # N-step Learning\n","        n_step: int = 3,\n","    ):\n","        \"\"\"\n","        Initialization.\n","\n","        Args:\n","            env (gym.Env): openAI Gym environment\n","            memory_size (int): length of memory\n","            batch_size (int): batch size for sampling\n","            target_update (int): period for target model's hard update\n","            epsilon_decay (float): step size to decrease epsilon\n","            lr (float): learning rate\n","            max_epsilon (float): max value of epsilon\n","            min_epsilon (float): min value of epsilon\n","            gamma (float): discount factor\n","            alpha (float)    : determines how much prioritization is used\n","            beta (float)     : determines how much importance sampling is used\n","            prior_eps (float): guarantees every transition can be sampled\n","            n_step (int): step number to calculate n-step td error\n","        \"\"\"\n","        self.env = env\n","        # network parameters\n","        self.state_size = self.env.observation_space.shape[0]\n","        self.action_size = self.env.action_space.n\n","\n","        # hyperparameters\n","        self.batch_size = batch_size\n","        self.epsilon = max_epsilon\n","        self.epsilon_decay = epsilon_decay\n","        self.max_epsilon = max_epsilon\n","        self.min_epsilon = min_epsilon\n","        self.target_update = target_update\n","        self.gamma = gamma\n","\n","\n","        # device: cpu / gpu\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(self.device)\n","\n","        # PER\n","        # memory for 1-step Learning\n","        self.beta = beta\n","        self.prior_eps = prior_eps\n","        self.memory = PrioritizedReplayBuffer(\n","            self.state_size, memory_size, batch_size, alpha\n","        )\n","\n","\n","        self.n_step = n_step\n","        self.memory_n = ReplayBuffer(\n","            self.state_size,\n","            memory_size,\n","            batch_size,\n","            n_step=n_step,\n","            gamma=gamma\n","        )\n","\n","        # networks: dqn, dqn_target\n","        self.dqn = Network(self.state_size, self.action_size\n","                          ).to(self.device)\n","        self.dqn_target = Network(self.state_size, self.action_size\n","                          ).to(self.device)\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n","        self.dqn_target.eval()\n","\n","        # optimizer\n","        self.optimizer = optim.Adam(self.dqn.parameters())\n","\n","        # transition to store in memory\n","        self.transition = list()\n","\n","        # mode: train / test\n","        self.is_test = False\n","\n","    def get_action(self, state: np.ndarray) -> np.ndarray:\n","        \"\"\"Select an action from the input state.\"\"\"\n","        # epsilon greedy policy\n","        if self.epsilon > np.random.random():\n","            selected_action = self.env.action_space.sample()\n","        else:\n","            selected_action = self.dqn(\n","                torch.FloatTensor(state).to(self.device)\n","            ).argmax()\n","            selected_action = selected_action.detach().cpu().numpy()\n","\n","        if not self.is_test:\n","            self.transition = [state, selected_action]\n","\n","        return selected_action\n","\n","    def _compute_dqn_loss(\n","        self, samples: Dict[str, np.ndarray], gamma: float\n","    ) -> torch.Tensor:\n","        \"\"\"Return dqn loss.\"\"\"\n","        device     = self.device  # for shortening the following lines\n","\n","        state      = torch.FloatTensor(samples[\"obs\"]).to(device)\n","        action     = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n","        reward     = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n","        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n","        done       = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n","\n","        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n","        #       = r                       otherwise\n","        curr_Qs = self.dqn(state).gather(1, action)\n","        next_Q_targs = self.dqn_target(\n","            next_state\n","        ).max(dim=1, keepdim=True)[0].detach()\n","        mask = 1 - done\n","        target_value = (reward + self.gamma * next_Q_targs * mask).to(self.device)\n","\n","        # calculate element-wise dqn loss\n","        elementwise_loss = F.smooth_l1_loss(curr_Qs, target_value, reduction=\"none\")\n","\n","        return elementwise_loss\n","\n","    def train_step(self) -> torch.Tensor:\n","        \"\"\"Update the model by gradient descent.\"\"\"\n","        # PER needs beta to calculate weights\n","\n","        samples = self.memory.sample_batch(self.beta)\n","        weights = torch.FloatTensor(samples[\"weights\"].reshape(-1, 1)).to(self.device)\n","        indices = samples[\"indices\"]\n","\n","        # 1-step Learning loss\n","        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n","\n","        # PER: importance sampling before average\n","        loss = torch.mean(elementwise_loss * weights)\n","\n","        # N-step Learning loss\n","        # we are gonna combine 1-step loss and n-step loss so as to\n","        # prevent high-variance. The original rainbow employs n-step loss only.\n","        gamma = self.gamma ** self.n_step\n","        samples = self.memory_n.sample_batch_from_idxs(indices)\n","        elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n","        elementwise_loss += elementwise_loss_n_loss\n","\n","        # PER: importance sampling before average\n","        loss = torch.mean(elementwise_loss * weights)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(self.dqn.parameters(), 10.0)\n","        self.optimizer.step()\n","\n","        # PER: update priorities\n","        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n","        new_priorities = loss_for_prior + self.prior_eps\n","        self.memory.update_priorities(indices, new_priorities)\n","\n","        return loss.item()\n","\n","    def _target_hard_update(self):\n","        \"\"\"Hard update: target <- local.\"\"\"\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n","\n","# environment\n","env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","\n","memory_size = 2000\n","target_update = 100\n","epsilon_decay = 1 / 2000\n","initial_random_steps = 5000\n","max_episodes = 100\n","batch_size = 32\n","\n","agent = DQNAgent(\n","    env,\n","    memory_size,\n","    batch_size,\n","    target_update,\n","    epsilon_decay,\n",")\n","\n","if __name__ == \"__main__\":\n","    agent.is_test = False\n","\n","    update_cnt    = 0\n","    epsilons      = []\n","    losses        = []\n","    scores        = []\n","    frame_idx = 0\n","    num_frames= 100000\n","\n","    # EACH EPISODE\n","    for episode in range(max_episodes):\n","        state = agent.env.reset()\n","        episode_reward = 0\n","        done = False\n","\n","        while not done:\n","            action = agent.get_action(state)\n","            next_state, reward, done, _ = agent.env.step(action)\n","            agent.transition += [reward, next_state, done]\n","            step_transition = agent.memory_n.store(*agent.transition)\n","\n","            # add a single step transition\n","            if step_transition:\n","                agent.memory.store(*step_transition)\n","\n","            state = next_state\n","            episode_reward += reward\n","\n","            frame_idx += 1\n","\n","            # PER: increase beta\n","            fraction = min(frame_idx / num_frames, 1.0)\n","            agent.beta = agent.beta + fraction * (1.0 - agent.beta)\n","\n","            # if episode ends\n","            if done:\n","                state = agent.env.reset()\n","                scores.append(episode_reward)\n","                print(\"Episode \" + str(episode+1) + \": \" + str(episode_reward))\n","\n","            # if training is ready\n","            if (len(agent.memory) >= agent.batch_size):\n","                loss = agent.train_step()\n","                losses.append(loss)\n","                update_cnt += 1\n","\n","                # linearly decrease epsilon\n","                agent.epsilon = max(\n","                    agent.min_epsilon, agent.epsilon - (\n","                        agent.max_epsilon - agent.min_epsilon\n","                    ) * agent.epsilon_decay\n","                )\n","                epsilons.append(agent.epsilon)\n","\n","                # if hard update is needed\n","                if update_cnt % agent.target_update == 0:\n","                    agent._target_hard_update()\n","\n"]},{"cell_type":"code","source":[],"metadata":{"id":"NdbieQJO3K5R"},"execution_count":null,"outputs":[]}]}