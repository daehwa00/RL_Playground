{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPbqHWXkHUSRdJOTaQBPQpR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DHsKDghzSozi"},"outputs":[],"source":["  class SumTree:\n","      data_pointer = 0\n","\n","      def __init__(self, capacity):\n","          self.capacity = capacity  # leaf node의 수 = capacity\n","          self.tree = np.zeros(2 * capacity - 1)  # 총 node의 수 -> 우선순위(priority)를 저장\n","          self.data = np.zeros(capacity, dtype=object)  # 경험(state, action, reward, next state, done flag로 이루어진 tuple)을 저장\n","          self.n_entries = 0\n","\n","      def add(self, priority, data):\n","          tree_index = self.data_pointer + self.capacity - 1\n","          self.data[self.data_pointer] = data # update data 프레임\n","          self.update(tree_index, priority) # leaf(priority) 업데이트\n","          self.data_pointer += 1  # pointer를 1 증가시킴\n","          if self.data_pointer >= self.capacity:  # capacity를 넘었다면 첫번째 index로 돌아감\n","              self.data_pointer = 0\n","          if self.n_entries < self.capacity:\n","              self.n_entries += 1\n","\n","      # leaf priority score 업데이트\n","      def _propagate(self, idx, change):\n","          parent = (idx - 1) // 2\n","          self.tree[parent] += change\n","          # parent가 0이면 중단. root node에 도달했기 때문\n","          if parent != 0:\n","              self._propagate(parent, change)\n","\n","      def update(self, tree_index, priority):\n","          change = priority - self.tree[tree_index]\n","          self.tree[tree_index] = priority\n","          self._propagate(tree_index, change)\n","\n","      # 이진 트리 구조를 사용하여 특정 조건을 만족하는 노드를 찾는 재귀 함수(recursive function)\n","      # 주어진 값 s에 대해 특정 조건을 만족하는 노드의 인덱스를 찾아라.\n","      def _retrieve(self, idx, s):\n","          left_child_index = 2 * idx + 1\n","          right_child_index = left_child_index + 1\n","          # 현재 노드가 leaf node(자식이 없는 node)인 경우를 검\n","          if left_child_index >= len(self.tree):\n","              return idx\n","          # s가 왼쪽 자식 노드에 저장된 값보다 작거나 같으면, 왼쪽 자식으로 재귀적으로 이동\n","          if s <= self.tree[left_child_index]:\n","              return self._retrieve(left_child_index, s)\n","          else:\n","              return self._retrieve(right_child_index, s - self.tree[left_child_index])\n","\n","      def get_leaf(self, s):\n","          leaf_index = self._retrieve(0, s)\n","          data_index = leaf_index - self.capacity + 1\n","          return (leaf_index, self.tree[leaf_index], self.data[data_index])\n","\n","      # 루트 노드를 반환\n","      def total_priority(self):\n","          return self.tree[0]\n"]},{"cell_type":"code","source":["class PrioritizedReplayBuffer(object):\n","    PER_e = 0.001 # 어떤 경험을 할 확률이 0이 되지 않도록 하는 hyperparameter\n","    PER_a = 0.6 # 우선순위가 높은 것과 무작위 샘플링 사이 절충을 하기 위한 hyperparameter\n","    PER_b = 0.4 # Importance Sampling. 1까지 증가\n","    PER_b_increment_per_sampling = 0.001\n","\n","    def __init__(self, capacity):\n","        self.tree = SumTree(capacity)\n","        self.capacity = capacity\n","\n","    # 최대 우선 순위 검색\n","    def _getPriority(self, error):\n","        return (error + self.PER_e) ** self.PER_a\n","\n","    def store(self, error, sample):\n","        max_priority = self._getPriority(error)\n","        self.tree.add(max_priority, sample)\n","\n","    def sample(self, n):\n","        minibatch = []\n","        idxs = []\n","        priority_segment = self.tree.total_priority() / n\n","        priorities = []\n","        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])\n","\n","        for i in range(n):\n","            a = priority_segment * i\n","            b = priority_segment * (i + 1)\n","            value = np.random.uniform(a, b)\n","            (idx, p, data) = self.tree.get_leaf(value)\n","            priorities.append(p)\n","            minibatch.append(data)\n","            idxs.append(idx)\n","\n","        sampling_probabilities = priorities / self.tree.total_priority()\n","        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.PER_b)\n","        is_weight /= is_weight.max()\n","\n","        return minibatch, idxs, is_weight\n","\n","    def batch_update(self, idx, error):\n","        p = self._getPriority(error)\n","        self.tree.update(idx, p)\n"],"metadata":{"id":"XGslHTPmVtNs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def append_sample(self, state, action, reward, next_state, done):\n","    # PyTorch 텐서로 변환\n","    state = torch.FloatTensor(state).unsqueeze(0)\n","    next_state = torch.FloatTensor(next_state).unsqueeze(0)\n","    action = torch.LongTensor([action])\n","    reward = torch.FloatTensor([reward])\n","    done = torch.FloatTensor([done])\n","\n","    # Q 값 계산\n","    with torch.no_grad():\n","        main_next_q = self.dqn(next_state)\n","        next_action = main_next_q.max(1)[1].view(1, 1)\n","        target_next_q = self.dqn_target(next_state)\n","        target_value = target_next_q.gather(1, next_action).item()\n","\n","    target_value = reward + (self.gamma * target_value * (1 - done))\n","\n","    # 현재 상태에 대한 Q 값\n","    main_q = self.dqn(state).gather(1, action.unsqueeze(1)).item()\n","\n","    # TD 오차 계산\n","    td_error = abs(target_value - main_q)\n","\n","    # 메모리에 경험 저장\n","    self.MEMORY.store(td_error, (state, action, reward, next_state, done))\n"],"metadata":{"id":"kH7aPmp4WiiT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_step(self):\n","    mini_batch, idxs, IS_weights = self.MEMORY.sample(self.batch_size)\n","\n","    states, actions, rewards, next_states, dones = zip(*mini_batch)\n","    states = torch.FloatTensor(states)\n","    actions = torch.LongTensor(actions)\n","    rewards = torch.FloatTensor(rewards)\n","    next_states = torch.FloatTensor(next_states)\n","    dones = torch.FloatTensor(dones)\n","\n","    # Q 값 계산 및 TD 오차 업데이트\n","    current_q_values = self.dqn(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n","    with torch.no_grad():\n","        max_next_q_values = self.dqn_target(next_states).max(1)[0]\n","    expected_q_values = rewards + (self.gamma * max_next_q_values * (1 - dones))\n","\n","    # 오차 계산\n","    td_errors = abs(current_q_values - expected_q_values)\n","\n","    # 손실 계산 (IS 가중치 적용)\n","    loss = (td_errors.pow(2) * torch.FloatTensor(IS_weights)).mean()\n","\n","    # 역전파\n","    self.optimizer.zero_grad()\n","    loss.backward()\n","    self.optimizer.step()\n","\n","    # 메모리 우선순위 업데이트\n","    for i in range(self.batch_size):\n","        idx = idxs[i]\n","        self.MEMORY.batch_update(idx, td_errors[i].item())\n"],"metadata":{"id":"9hioqEDYWjxx"},"execution_count":null,"outputs":[]}]}