{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5135,"status":"ok","timestamp":1700553116235,"user":{"displayName":"고대화","userId":"04859921805535355794"},"user_tz":-540},"id":"kU-JrEzmRwVx"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","import gym\n","import numpy as np\n","\n","class Actor(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super(Actor, self).__init__()\n","        self.fc1 = nn.Linear(state_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.policy = nn.Linear(hidden_size, action_size)\n","\n","        self.action_size = action_size\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return F.softmax(self.policy(x), dim=-1)\n","\n","    def compute_loss(self, old_policy, new_policy, actions, gaes):\n","        gaes = gaes.detach()\n","        old_log_p = torch.log(torch.sum(old_policy * actions, dim=1))\n","        old_log_p = old_log_p.detach()\n","        log_p = torch.log(torch.sum(new_policy * actions, dim=1))\n","        ratios = torch.exp(log_p - old_log_p)\n","        clipped_ratios = torch.clamp(ratios, 1 - clip_ratio, 1 + clip_ratio)\n","        surrogate = -torch.min(ratios * gaes, clipped_ratios * gaes)\n","        return surrogate.mean()\n","\n","    def train(self, optimizer, old_policy, states, actions, gaes):\n","        actions = F.one_hot(actions.long(), num_classes=self.action_size)\n","        actions = actions.float()\n","\n","        optimizer.zero_grad()\n","        curr_policy = self(states)\n","        loss = self.compute_loss(old_policy, curr_policy, actions, gaes)\n","        loss.backward()\n","        optimizer.step()\n","        return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QJx8CxcpSJiB"},"outputs":[],"source":["class Critic(nn.Module):\n","    def __init__(self, state_size):\n","        super(Critic, self).__init__()\n","        self.fc1 = nn.Linear(state_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, 1)\n","\n","    def forward(self, x):\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        return self.value(x)\n","\n","    def compute_loss(self, v_pred, td_targets):\n","        # v_pred의 차원을 [n, 1]에서 [n]으로 변경\n","        v_pred = v_pred.squeeze(-1)\n","        # td_targets의 차원이 스칼라인 경우 [1]로 변경\n","        td_targets = td_targets.unsqueeze(-1) if td_targets.dim() == 0 else td_targets\n","        return F.mse_loss(v_pred, td_targets)\n","\n","    def train(self, optimizer, states, td_targets):\n","        optimizer.zero_grad()\n","        v_pred = self(states)\n","        loss = self.compute_loss(v_pred, td_targets.detach())\n","        loss.backward()\n","        optimizer.step()\n","        return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"0FHUMH0BR46K","outputId":"d73abcd1-3606-4bb6-c175-bff3b0d76a1d"},"outputs":[{"name":"stderr","output_type":"stream","text":["Training Progress: 100%|██████████| 500/500 [03:38<00:00,  2.29it/s, Episode_Reward=200]\n"]}],"source":["from tqdm import tqdm\n","\n","class PPOAgent:\n","    def __init__(self, env_name, gamma):\n","        self.env = gym.make(env_name)\n","\n","        self.state_size = self.env.observation_space.shape[0]\n","        self.action_size = self.env.action_space.n\n","\n","        self.gamma = gamma\n","\n","        self.actor = Actor(self.state_size, self.action_size)\n","        self.critic = Critic(self.state_size)\n","\n","        self.actor_opt = optim.Adam(self.actor.parameters(), lr=actor_lr)\n","        self.critic_opt = optim.Adam(self.critic.parameters(), lr=critic_lr)\n","\n","    def gae_target(self, rewards, curr_Qs, next_Q, done):\n","        td_targets = np.zeros_like(rewards)\n","        gae = np.zeros_like(rewards)\n","        gae_cumulative = 0\n","        future_reward = 0\n","\n","        if not done:\n","            future_reward = next_Q\n","\n","        for k in reversed(range(0, len(rewards))):\n","            delta = rewards[k] + self.gamma * future_reward - curr_Qs[k]\n","            gae_cumulative = self.gamma * lmbda * gae_cumulative + delta\n","            gae[k] = gae_cumulative\n","            future_reward = curr_Qs[k]\n","            td_targets[k] = gae[k] + curr_Qs[k]\n","        return gae, td_targets\n","\n","    def train(self, max_episodes, update_interval):\n","        progress_bar = tqdm(range(max_episodes), desc=\"Training Progress\")\n","        for episode in progress_bar:\n","            episode_reward = 0\n","            done = False\n","            state = self.env.reset()\n","\n","            states = []\n","            actions = []\n","            rewards = []\n","            old_policys = []\n","\n","            while not done:\n","                state_tensor = torch.tensor(state, dtype=torch.float32)\n","                probs = self.actor(state_tensor).detach().numpy()\n","                action = np.random.choice(self.action_size, p=probs)\n","\n","                next_state, reward, done, _ = self.env.step(action)\n","\n","                states.append(state)\n","                actions.append(action)\n","                rewards.append(reward)\n","                old_policys.append(probs)\n","\n","                state = next_state\n","                episode_reward += reward\n","\n","                if len(states) >= update_interval or done:\n","                    states_tensor = torch.tensor(states, dtype=torch.float32)\n","                    actions_tensor = torch.tensor(actions, dtype=torch.long)\n","                    rewards_tensor = torch.tensor(rewards, dtype=torch.float32)\n","                    old_policys_tensor = torch.tensor(old_policys, dtype=torch.float32)\n","\n","                    curr_Qs = self.critic(states_tensor).detach().numpy()\n","                    next_Q = self.critic(torch.tensor(next_state, dtype=torch.float32)).item()\n","\n","                    gaes, td_targets = self.gae_target(rewards, curr_Qs, next_Q, done)\n","                    gaes = torch.tensor(gaes, dtype=torch.float32)\n","                    td_targets = torch.tensor(td_targets, dtype=torch.float32)\n","\n","                    for epoch in range(epochs):\n","                        actor_loss = self.actor.train(self.actor_opt, old_policys_tensor, states_tensor, actions_tensor, gaes)\n","                        critic_loss = self.critic.train(self.critic_opt, states_tensor, td_targets)\n","\n","                    states = []\n","                    actions = []\n","                    rewards = []\n","                    old_policys = []\n","\n","                progress_bar.set_postfix({\"Episode_Reward\": episode_reward})\n","\n","if __name__ == \"__main__\":\n","\n","    env_name = \"CartPole-v0\"\n","    actor_lr = 0.0005\n","    critic_lr = 0.001\n","    gamma = 0.99\n","    hidden_size = 128\n","    update_interval = 50\n","    clip_ratio = 0.1\n","    lmbda = 0.95\n","    epochs = 7\n","    max_episodes = 500\n","\n","    agent = PPOAgent(env_name, gamma)\n","    agent.train(max_episodes, update_interval)\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMwieiHPXtbK2Y5xhf+M/Xk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}