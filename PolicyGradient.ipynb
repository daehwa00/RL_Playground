{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNZtRxn2k+IAkfmxN+/98if"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":15,"metadata":{"id":"LOCRqIEYOYW7","executionInfo":{"status":"ok","timestamp":1700128046832,"user_tz":-540,"elapsed":2,"user":{"displayName":"고대화","userId":"04859921805535355794"}}},"outputs":[],"source":["import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.distributions import Categorical\n","from tqdm import tqdm\n","\n","class Network(nn.Module):\n","    def __init__(self, state_size, action_size, hidden_size):\n","        super(Network, self).__init__()\n","        self.fc1 = nn.Linear(state_size, hidden_size)\n","        self.fc2 = nn.Linear(hidden_size, hidden_size)\n","        self.policy = nn.Linear(hidden_size, action_size)\n","        self.softmax = nn.Softmax(dim=-1)\n","\n","    def forward(self, x):\n","        x = torch.relu(self.fc1(x))\n","        x = torch.relu(self.fc2(x))\n","        x = self.softmax(self.policy(x))\n","        return x\n"]},{"cell_type":"code","source":["\n","class PolicyGradient:\n","    def __init__(self, env, state_size, action_size, hidden_size, lr, gamma):\n","        self.env = env\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.lr = lr\n","        self.gamma = gamma\n","        self.model = Network(state_size, action_size, hidden_size)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=lr)\n","\n","    def get_action(self, state):\n","        state = torch.from_numpy(state).float().unsqueeze(0)\n","        probs = self.model(state)\n","        m = Categorical(probs)\n","        action = m.sample()\n","        return action.item()\n","\n","    def compute_loss(self, states, actions, rewards):\n","        states = torch.FloatTensor(states)\n","        actions = torch.LongTensor(actions)\n","        rewards = torch.FloatTensor(rewards)\n","\n","        probs = self.model(states)\n","        m = Categorical(probs)\n","        log_probs = m.log_prob(actions)\n","        loss = -(log_probs * rewards).mean()\n","        return loss\n","\n","    def train_step(self, states, actions, rewards):\n","        self.optimizer.zero_grad()\n","        loss = self.compute_loss(states, actions, rewards)\n","        loss.backward()\n","        self.optimizer.step()\n","        return loss.item()\n"],"metadata":{"id":"yNHm2IOmWDbG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Parameters\n","env_name = \"CartPole-v0\"\n","hidden_size = 64\n","max_episodes = 500\n","lr = 7e-3\n","gamma = 0.99\n","seed = 12345\n"],"metadata":{"id":"FbEEvjPphuJt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Environment\n","env = gym.make(env_name)\n","env.seed(seed)\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n"],"metadata":{"id":"X7Ss0a8Jhu9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Agent\n","agent = PolicyGradient(env, state_size, action_size, hidden_size, lr, gamma)\n","\n","if __name__ == \"__main__\":\n","    torch.manual_seed(seed)\n","    scores = []\n","    progress_bar = tqdm(range(max_episodes), desc=\"Training Progress\")\n","    for episode in progress_bar:\n","        state = env.reset()\n","        episode_reward = 0\n","        done = False\n","\n","        states = []\n","        actions = []\n","        rewards = []\n","\n","        while not done:\n","            action = agent.get_action(state)\n","            next_state, reward, done, _ = env.step(action)\n","\n","            states.append(state)\n","            actions.append(action)\n","            rewards.append(reward)\n","\n","            state = next_state\n","            episode_reward += reward\n","\n","            if done:\n","                scores.append(episode_reward)\n","                progress_bar.set_postfix({'Episode': episode + 1, 'Reward': episode_reward})\n","\n","                # Discount rewards (revised)\n","                R = 0\n","                discounted_rewards = []\n","                for r in rewards[::-1]:\n","                    R = r + gamma * R\n","                    discounted_rewards.insert(0, R)\n","                discounted_rewards = torch.tensor(discounted_rewards)\n","\n","                loss = agent.train_step(states, actions, discounted_rewards)\n","                break\n"],"metadata":{"id":"7xTa9J3ghv0R"},"execution_count":null,"outputs":[]}]}