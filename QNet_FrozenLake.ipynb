{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOkVWKLZcsMDK7VLtV+B6yy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"id":"66I-Ixx7OYMb","executionInfo":{"status":"ok","timestamp":1699674629868,"user_tz":-540,"elapsed":283,"user":{"displayName":"고대화","userId":"04859921805535355794"}}},"outputs":[],"source":["import gym\n","import numpy as np\n","import random\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.autograd import Variable\n","\n","# 3.2 Python function for one hot encoding\n","def to_one_hot(i, n_classes=None):\n","    a = np.zeros(n_classes, 'uint8')\n","    a[i] = 1\n","    return a\n","\n","# 3.3 CREATING THE Q-Network\n","# Neural Network Model Defined at Here.\n","class Network(nn.Module):\n","    def __init__(self, state_size: int, action_size: int):\n","        \"\"\"Initialization.\"\"\"\n","        super(Network, self).__init__()\n","\n","        self.layer1 = nn.Linear(state_size, hidden_size)\n","        self.layer2 = nn.Linear(hidden_size, hidden_size)\n","        self.value = nn.Linear(hidden_size, action_size)\n","\n","    def forward(self, state):\n","        if not isinstance(state, torch.Tensor):\n","            state = torch.from_numpy(state).float()\n","        layer1 = torch.relu(self.layer1(state))\n","        layer2 = torch.relu(self.layer2(layer1))\n","        value = self.value(layer2)\n","        return value\n","\n","\n","class DQNAgent:\n","    def __init__(self, env: gym.Env):\n","        \"\"\"Initialization.\"\"\"\n","        self.env = env\n","\n","        self.state_size  = env.observation_space.n\n","        self.action_size = env.action_space.n\n","\n","        self.lr = 0.001\n","        self.gamma = 0.99\n","\n","        self.dqn = Network(self.state_size, self.action_size)\n","        self.optimizer = optim.Adam(self.dqn.parameters(), lr=self.lr)\n","\n","    # 3.4.1 EXPLORATION VS EXPLOITATION\n","    def get_action(self, state, epsilon):\n","        state = to_one_hot(state, self.state_size)\n","        if random.random() <= epsilon:\n","            return random.choice(range(self.action_size))\n","        else:\n","            state = torch.from_numpy(state).float().unsqueeze(0)\n","            q_values = self.dqn(state)\n","            return torch.argmax(q_values).item()\n","\n","    # 3.4.2 UPDATING THE Q-VALUE\n","    def train_step(self, state, action, reward, next_state, done):\n","        state = torch.from_numpy(to_one_hot(state, self.state_size)).float()\n","        next_state = torch.from_numpy(to_one_hot(next_state, self.state_size)).float()\n","        action = torch.tensor(action)\n","        reward = torch.tensor(reward)\n","\n","        if done:\n","            target = reward\n","        else:\n","            next_state_values = self.dqn(next_state).detach()\n","            target = reward + self.gamma * torch.max(next_state_values)\n","\n","        predicted_value = self.dqn(state)[action]\n","\n","        loss = torch.nn.functional.mse_loss(predicted_value, target)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n"]},{"cell_type":"code","source":["import gym\n","import numpy as np\n","import torch\n","from tqdm import tqdm\n","\n","# DQNAgent 클래스와 필요한 모듈들을 정의하는 부분은 여기에 추가해야 합니다.\n","\n","# 2.2 환경 생성\n","env_name = \"FrozenLake-v1\"\n","env = gym.make(env_name)\n","env.seed(1)  # 재현 가능성을 위해\n","\n","# 2.4 하이퍼파라미터 초기화\n","hidden_size = 128\n","max_episodes = 2500  # 총 에피소드 수\n","max_steps = 99       # 에피소드당 최대 스텝 수\n","gamma = 0.95         # 할인율\n","render = False       # 게임 환경 표시 여부\n","\n","# 탐험 파라미터\n","epsilon = 1.0        # 탐험률\n","max_epsilon = 1.0    # 시작시 탐험 확률\n","min_epsilon = 0.01   # 최소 탐험 확률\n","decay_rate = 0.005   # 탐험 확률의 지수 감소율\n","\n","# 에이전트 훈련\n","agent = DQNAgent(env)\n","\n","if __name__ == \"__main__\":\n","    scores = []\n","\n","    with tqdm(total=max_episodes, desc=\"에피소드 진행\") as pbar:\n","        for episode in range(max_episodes):\n","            state = agent.env.reset()\n","            episode_reward = 0\n","            done = False\n","\n","            if render: env.render()\n","\n","            while not done:\n","                action = agent.get_action(state, epsilon)\n","                next_state, reward, done, _ = agent.env.step(action)\n","\n","                if render: env.render()\n","\n","                agent.train_step(state, action, reward, next_state, done)\n","\n","                state = next_state\n","                episode_reward += reward\n","\n","                if done:\n","                    scores.append(episode_reward)\n","                    pbar.set_postfix({'episode_reward': episode_reward})\n","                    pbar.update(1)\n","                    break\n","\n","            epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n","\n","    print(f\"평균 점수: {sum(scores) / max_episodes}\")\n","\n","    count = 500\n","    rewards_per_thousand_episodes = np.split(np.array(scores), int(max_episodes / 500))\n","\n","    print(\"********천 개 에피소드당 평균 보상********\\n\")\n","    for r in rewards_per_thousand_episodes:\n","        print(f\"{count}: {sum(r) / 500}\")\n","        count += 500\n"],"metadata":{"id":"b9NlijnLOkFJ","executionInfo":{"status":"ok","timestamp":1699675311973,"user_tz":-540,"elapsed":233542,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"00a5811c-6d75-481f-d41a-fecd2225edf4","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["에피소드 진행: 100%|██████████| 2500/2500 [03:53<00:00, 10.71it/s, episode_reward=0]"]},{"output_type":"stream","name":"stdout","text":["평균 점수: 0.4356\n","********천 개 에피소드당 평균 보상********\n","\n","500: 0.112\n","1000: 0.366\n","1500: 0.57\n","2000: 0.504\n","2500: 0.626\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]}]}