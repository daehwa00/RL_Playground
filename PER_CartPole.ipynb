{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMAQgFo5p92xu3L/7MtzAG2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"plmFKYc0X2qn","executionInfo":{"status":"ok","timestamp":1699928192687,"user_tz":-540,"elapsed":4,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"f96ba633-415b-42cf-fbdc-fe8442d99303"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.00118216,  0.04504637, -0.03558404,  0.04486495], dtype=float32)"]},"metadata":{},"execution_count":93}],"source":["import gym\n","\n","env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","env.reset(seed=1)"]},{"cell_type":"code","source":["class SumTree:\n","    data_pointer = 0\n","\n","    def __init__(self, capacity):\n","        self.capacity = capacity  # leaf node의 수 = capacity\n","        self.tree = np.zeros(2 * capacity - 1)  # 총 node의 수 -> 우선순위(priority)를 저장\n","        self.data = np.zeros(capacity, dtype=object)  # 경험(state, action, reward, next state, done flag로 이루어진 tuple)을 저장\n","        self.n_entries = 0\n","\n","    def add(self, priority, data):\n","        tree_index = self.data_pointer + self.capacity - 1\n","        self.data[self.data_pointer] = data # update data 프레임\n","        self.update(tree_index, priority) # leaf(priority) 업데이트\n","        self.data_pointer += 1  # pointer를 1 증가시킴\n","        if self.data_pointer >= self.capacity:  # capacity를 넘었다면 첫번째 index로 돌아감\n","            self.data_pointer = 0\n","        if self.n_entries < self.capacity:\n","            self.n_entries += 1\n","\n","    # leaf priority score 업데이트\n","    def _propagate(self, idx, change):\n","        parent = (idx - 1) // 2\n","        self.tree[parent] += change\n","        if parent != 0:\n","            self._propagate(parent, change)\n","\n","    def update(self, tree_index, priority):\n","        change = priority - self.tree[tree_index]\n","        self.tree[tree_index] = priority\n","        self._propagate(tree_index, change)\n","\n","    def _retrieve(self, idx, s):\n","        left_child_index = 2 * idx + 1\n","        right_child_index = left_child_index + 1\n","        if left_child_index >= len(self.tree):\n","            return idx\n","        if s <= self.tree[left_child_index]:\n","            return self._retrieve(left_child_index, s)\n","        else:\n","            return self._retrieve(right_child_index, s - self.tree[left_child_index])\n","\n","    def get_leaf(self, s):\n","        leaf_index = self._retrieve(0, s)\n","        data_index = leaf_index - self.capacity + 1\n","        return (leaf_index, self.tree[leaf_index], self.data[data_index])\n","\n","    # 루트 노드를 반환\n","    def total_priority(self):\n","        return self.tree[0]\n","\n","class PrioritizedReplayBuffer(object):\n","    PER_e = 0.001 # 어떤 경험을 할 확률이 0이 되지 않도록 하는 hyperparameter\n","    PER_a = 0.6 # 우선순위가 높은 것과 무작위 샘플링 사이 절충을 하기 위한 hyperparameter\n","    PER_b = 0.4 # Importance Sampling. 1까지 증가\n","    PER_b_increment_per_sampling = 0.001\n","\n","    def __init__(self, capacity):\n","        self.tree = SumTree(capacity)\n","        self.capacity = capacity\n","\n","    # 최대 우선 순위 검색\n","    def _getPriority(self, error):\n","        return (error + self.PER_e) ** self.PER_a\n","\n","    def store(self, error, sample):\n","        max_priority = self._getPriority(error)\n","        self.tree.add(max_priority, sample)\n","\n","    def sample(self, n):\n","        minibatch = []\n","        idxs = []\n","        priority_segment = self.tree.total_priority() / n\n","        priorities = []\n","        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])\n","\n","        for i in range(n):\n","            a = priority_segment * i\n","            b = priority_segment * (i + 1)\n","            value = np.random.uniform(a, b)\n","            (idx, p, data) = self.tree.get_leaf(value)\n","            priorities.append(p)\n","            minibatch.append(data)\n","            idxs.append(idx)\n","\n","        sampling_probabilities = priorities / self.tree.total_priority()\n","        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.PER_b)\n","        is_weight /= is_weight.max()\n","\n","        return minibatch, idxs, is_weight\n","\n","    def batch_update(self, idxs, errors):\n","        for i, error in zip(idxs, errors):\n","            p = self._getPriority(error)\n","            self.tree.update(i, p)\n","\n"],"metadata":{"id":"S0lDljwGYDsr","executionInfo":{"status":"ok","timestamp":1699928756682,"user_tz":-540,"elapsed":2,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5dcb77c5-19c9-4ae2-e676-044fa522be20"},"execution_count":129,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n","  and should_run_async(code)\n"]}]},{"cell_type":"markdown","source":["### Q-Network"],"metadata":{"id":"0MO98nlUYSa-"}},{"cell_type":"code","source":["# INITIALIZING THE Q-PARAMETERS\n","hidden_size = 128\n","max_episodes = 200  # Set total number of episodes to train agent on.\n","batch_size = 64\n","\n","# Exploration parameters\n","epsilon = 1.0                 # Exploration rate\n","max_epsilon = 1.0             # Exploration probability at start\n","min_epsilon = 0.01            # Minimum exploration probability\n","decay_rate = 0.005            # Exponential decay rate for exploration prob"],"metadata":{"id":"6V14459zaLlx","executionInfo":{"status":"ok","timestamp":1699928757141,"user_tz":-540,"elapsed":1,"user":{"displayName":"고대화","userId":"04859921805535355794"}}},"execution_count":130,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Network(nn.Module):\n","    def __init__(self, state_size, action_size, hidden_size):\n","        super(Network, self).__init__()\n","        self.layer1 = nn.Linear(state_size, hidden_size)\n","        self.layer2 = nn.Linear(hidden_size, hidden_size)\n","        self.state = nn.Linear(hidden_size, action_size)\n","        self.action = nn.Linear(hidden_size, action_size)\n","\n","    def forward(self, state):\n","        layer1 = F.relu(self.layer1(state))\n","        layer2 = F.relu(self.layer2(layer1))\n","        state_value = self.state(layer2)\n","        action_advantage = self.action(layer2)\n","        mean = torch.mean(action_advantage, dim=1, keepdim=True)\n","        advantage = action_advantage - mean\n","        value = state_value + advantage\n","        return value\n"],"metadata":{"id":"kKm6_YwtYSHR","executionInfo":{"status":"ok","timestamp":1699928758167,"user_tz":-540,"elapsed":1,"user":{"displayName":"고대화","userId":"04859921805535355794"}}},"execution_count":131,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import numpy as np\n","import random\n","\n","class DQNAgent:\n","    def __init__(self, env, batch_size, target_update, hidden_size):\n","        self.env = env\n","        self.state_size = env.observation_space.shape[0]\n","        self.action_size = env.action_space.n\n","        self.batch_size = batch_size\n","        self.gamma = 0.99\n","        self.lr = 0.001\n","        self.target_update = target_update\n","        self.soft_update = False\n","        self.tau = 0.1\n","\n","        self.dqn = Network(self.state_size, self.action_size, hidden_size)\n","        self.dqn_target = Network(self.state_size, self.action_size, hidden_size)\n","        self.optimizer = optim.Adam(self.dqn.parameters(), lr=self.lr)\n","        self.memory = PrioritizedReplayBuffer(10000)  # Assuming PrioritizedReplayBuffer is defined\n","\n","        self._target_hard_update()\n","\n","    def get_action(self, state, epsilon):\n","        state = torch.FloatTensor(state).unsqueeze(0)\n","        q_value = self.dqn(state)\n","        if random.random() <= epsilon:\n","            action = random.choice(range(self.action_size))\n","        else:\n","            action = torch.argmax(q_value).item()\n","        return action\n","\n","    def append_sample(self, state, action, reward, next_state, done):\n","        state = torch.FloatTensor([state])\n","        next_state = torch.FloatTensor([next_state])\n","\n","        self.dqn.eval()\n","        self.dqn_target.eval()\n","        with torch.no_grad():\n","            next_Q = self.dqn(next_state)\n","            next_action = torch.argmax(next_Q, dim=1)\n","            target_next_Q = self.dqn_target(next_state)\n","            target_value = target_next_Q.gather(1, next_action.unsqueeze(1)).squeeze(1)\n","\n","        target_value = reward + self.gamma * target_value * (1 - done)\n","\n","        self.dqn.train()\n","        curr_Q = self.dqn(state)\n","        curr_Q = curr_Q.gather(1, torch.tensor([[action]])).squeeze(1)\n","\n","        td_error = torch.abs(target_value - curr_Q).item()\n","\n","        self.memory.store(td_error, (state, action, reward, next_state, done))\n","\n","    def train_step(self):\n","\n","        mini_batch, idxs, IS_weights = self.memory.sample(self.batch_size)\n","        states = np.array([np.array(x[0], dtype=np.float32) for x in mini_batch])\n","        states = torch.tensor(states)\n","        actions = torch.tensor([x[1] for x in mini_batch], dtype=torch.int64)\n","        rewards = torch.tensor([x[2] for x in mini_batch], dtype=torch.float32)\n","        next_states = torch.tensor([x[3].numpy() if isinstance(x[3], torch.Tensor) else x[3] for x in mini_batch], dtype=torch.float32)\n","        dones = torch.tensor([x[4] for x in mini_batch], dtype=torch.float32)\n","\n","\n","        # Q 값과 타겟 Q 값 계산\n","        q_values = self.dqn(states).squeeze(1)\n","        next_q_values = self.dqn_target(next_states).squeeze(1)\n","\n","\n","        # 여기서 q_values와 next_q_values는 [64, 2] 형태를 가져야 합니다.\n","        actions = actions.view(-1, 1)\n","        curr_Q = q_values.gather(1, actions).squeeze(1)\n","        next_Q = next_q_values.max(1)[0]\n","        expected_Q = rewards + self.gamma * next_Q * (1 - dones)\n","\n","\n","        # 손실 계산\n","        errors = torch.abs(curr_Q - expected_Q.detach())\n","        IS_weights = torch.tensor(IS_weights, dtype=torch.float32)\n","        loss = (IS_weights * F.mse_loss(curr_Q, expected_Q.detach(), reduction='none')).mean()\n","\n","        # 역전파와 옵티마이저를 통한 업데이트\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","        # 우선순위 업데이트\n","        new_priorities = errors.detach().numpy() + 1e-5\n","        self.memory.batch_update(idxs, new_priorities)\n","\n","\n","        return loss.item()\n","\n","    def _target_hard_update(self):\n","        if self.soft_update:\n","            for target_param, local_param in zip(self.dqn_target.parameters(), self.dqn.parameters()):\n","                target_param.data.copy_(self.tau * local_param.data + (1.0 - self.tau) * target_param.data)\n","        else:\n","            self.dqn_target.load_state_dict(self.dqn.state_dict())\n","\n","    def save(self, filepath):\n","        \"\"\"모델의 상태 사전을 파일에 저장합니다.\"\"\"\n","        torch.save(self.dqn.state_dict(), filepath)\n","\n","    def load(self, filepath):\n","        \"\"\"파일에서 모델의 상태 사전을 불러와 모델에 적용합니다.\"\"\"\n","        self.dqn.load_state_dict(torch.load(filepath))\n","        self.dqn.eval()  # 모델을 평가 모드로 설정"],"metadata":{"id":"2wBgHhbZZt_I","executionInfo":{"status":"ok","timestamp":1699928769452,"user_tz":-540,"elapsed":2,"user":{"displayName":"고대화","userId":"04859921805535355794"}}},"execution_count":134,"outputs":[]},{"cell_type":"code","source":["env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","\n","# 파라미터 설정\n","target_update = 20\n","hidden_size = 64\n","max_episodes = 300\n","batch_size = 64\n","\n","# 탐색 파라미터\n","epsilon = 1.0\n","max_epsilon = 1.0\n","min_epsilon = 0.01\n","decay_rate = 0.025\n","\n","# 에이전트 초기화\n","agent = DQNAgent(env, batch_size, target_update, hidden_size)\n","\n","update_cnt = 0\n","scores = []\n","\n","for episode in range(max_episodes):\n","    state = env.reset()\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        update_cnt += 1\n","        action = agent.get_action(np.array(state, dtype=np.float32), epsilon)\n","        next_state, reward, done, _ = env.step(action)\n","\n","        agent.append_sample(state, action, reward, next_state, done)\n","        state = next_state\n","        episode_reward += reward\n","\n","        if done:\n","            scores.append(episode_reward)\n","            print(f\"episode: {episode+1}/{max_episodes}, score: {episode_reward}, e: {epsilon:.4}\")\n","            break\n","\n","        if update_cnt >= agent.batch_size:\n","            agent.train_step()\n","\n","            if update_cnt % agent.target_update == 0:\n","                agent._target_hard_update()\n","\n","    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l-q1IOF9Z6lk","executionInfo":{"status":"ok","timestamp":1699929241776,"user_tz":-540,"elapsed":248811,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"ca46ba69-689e-40c7-e145-2b7305f97ade"},"execution_count":136,"outputs":[{"output_type":"stream","name":"stdout","text":["episode: 1/300, score: 45.0, e: 1.0\n","episode: 2/300, score: 12.0, e: 1.0\n","episode: 3/300, score: 23.0, e: 0.9756\n","episode: 4/300, score: 29.0, e: 0.9517\n","episode: 5/300, score: 25.0, e: 0.9285\n","episode: 6/300, score: 16.0, e: 0.9058\n","episode: 7/300, score: 10.0, e: 0.8837\n","episode: 8/300, score: 41.0, e: 0.8621\n","episode: 9/300, score: 14.0, e: 0.8411\n","episode: 10/300, score: 13.0, e: 0.8205\n","episode: 11/300, score: 20.0, e: 0.8005\n","episode: 12/300, score: 16.0, e: 0.781\n","episode: 13/300, score: 12.0, e: 0.762\n","episode: 14/300, score: 36.0, e: 0.7434\n","episode: 15/300, score: 17.0, e: 0.7253\n","episode: 16/300, score: 26.0, e: 0.7076\n","episode: 17/300, score: 20.0, e: 0.6904\n","episode: 18/300, score: 27.0, e: 0.6736\n","episode: 19/300, score: 23.0, e: 0.6572\n","episode: 20/300, score: 19.0, e: 0.6413\n","episode: 21/300, score: 48.0, e: 0.6257\n","episode: 22/300, score: 64.0, e: 0.6105\n","episode: 23/300, score: 46.0, e: 0.5956\n","episode: 24/300, score: 74.0, e: 0.5812\n","episode: 25/300, score: 25.0, e: 0.5671\n","episode: 26/300, score: 13.0, e: 0.5533\n","episode: 27/300, score: 54.0, e: 0.5399\n","episode: 28/300, score: 9.0, e: 0.5268\n","episode: 29/300, score: 108.0, e: 0.5141\n","episode: 30/300, score: 134.0, e: 0.5016\n","episode: 31/300, score: 112.0, e: 0.4895\n","episode: 32/300, score: 28.0, e: 0.4776\n","episode: 33/300, score: 113.0, e: 0.4661\n","episode: 34/300, score: 94.0, e: 0.4548\n","episode: 35/300, score: 60.0, e: 0.4439\n","episode: 36/300, score: 89.0, e: 0.4331\n","episode: 37/300, score: 61.0, e: 0.4227\n","episode: 38/300, score: 80.0, e: 0.4125\n","episode: 39/300, score: 76.0, e: 0.4026\n","episode: 40/300, score: 114.0, e: 0.3929\n","episode: 41/300, score: 156.0, e: 0.3834\n","episode: 42/300, score: 147.0, e: 0.3742\n","episode: 43/300, score: 134.0, e: 0.3652\n","episode: 44/300, score: 149.0, e: 0.3564\n","episode: 45/300, score: 168.0, e: 0.3479\n","episode: 46/300, score: 56.0, e: 0.3395\n","episode: 47/300, score: 170.0, e: 0.3314\n","episode: 48/300, score: 191.0, e: 0.3235\n","episode: 49/300, score: 168.0, e: 0.3157\n","episode: 50/300, score: 200.0, e: 0.3082\n","episode: 51/300, score: 194.0, e: 0.3008\n","episode: 52/300, score: 32.0, e: 0.2936\n","episode: 53/300, score: 200.0, e: 0.2866\n","episode: 54/300, score: 200.0, e: 0.2798\n","episode: 55/300, score: 200.0, e: 0.2731\n","episode: 56/300, score: 200.0, e: 0.2666\n","episode: 57/300, score: 200.0, e: 0.2603\n","episode: 58/300, score: 200.0, e: 0.2541\n","episode: 59/300, score: 200.0, e: 0.2481\n","episode: 60/300, score: 200.0, e: 0.2422\n","episode: 61/300, score: 200.0, e: 0.2365\n","episode: 62/300, score: 200.0, e: 0.2309\n","episode: 63/300, score: 200.0, e: 0.2254\n","episode: 64/300, score: 200.0, e: 0.2201\n","episode: 65/300, score: 29.0, e: 0.2149\n","episode: 66/300, score: 200.0, e: 0.2099\n","episode: 67/300, score: 200.0, e: 0.2049\n","episode: 68/300, score: 200.0, e: 0.2001\n","episode: 69/300, score: 200.0, e: 0.1954\n","episode: 70/300, score: 200.0, e: 0.1909\n","episode: 71/300, score: 200.0, e: 0.1864\n","episode: 72/300, score: 200.0, e: 0.182\n","episode: 73/300, score: 200.0, e: 0.1778\n","episode: 74/300, score: 200.0, e: 0.1736\n","episode: 75/300, score: 200.0, e: 0.1696\n","episode: 76/300, score: 200.0, e: 0.1657\n","episode: 77/300, score: 200.0, e: 0.1618\n","episode: 78/300, score: 200.0, e: 0.1581\n","episode: 79/300, score: 200.0, e: 0.1544\n","episode: 80/300, score: 200.0, e: 0.1509\n","episode: 81/300, score: 200.0, e: 0.1474\n","episode: 82/300, score: 200.0, e: 0.144\n","episode: 83/300, score: 200.0, e: 0.1407\n","episode: 84/300, score: 200.0, e: 0.1374\n","episode: 85/300, score: 200.0, e: 0.1343\n","episode: 86/300, score: 200.0, e: 0.1312\n","episode: 87/300, score: 200.0, e: 0.1282\n","episode: 88/300, score: 200.0, e: 0.1253\n","episode: 89/300, score: 200.0, e: 0.1225\n","episode: 90/300, score: 200.0, e: 0.1197\n","episode: 91/300, score: 200.0, e: 0.117\n","episode: 92/300, score: 200.0, e: 0.1143\n","episode: 93/300, score: 200.0, e: 0.1118\n","episode: 94/300, score: 200.0, e: 0.1093\n","episode: 95/300, score: 200.0, e: 0.1068\n","episode: 96/300, score: 200.0, e: 0.1044\n","episode: 97/300, score: 175.0, e: 0.1021\n","episode: 98/300, score: 155.0, e: 0.09981\n","episode: 99/300, score: 149.0, e: 0.09759\n","episode: 100/300, score: 162.0, e: 0.09543\n","episode: 101/300, score: 128.0, e: 0.09332\n","episode: 102/300, score: 140.0, e: 0.09126\n","episode: 103/300, score: 119.0, e: 0.08926\n","episode: 104/300, score: 119.0, e: 0.0873\n","episode: 105/300, score: 121.0, e: 0.08539\n","episode: 106/300, score: 127.0, e: 0.08353\n","episode: 107/300, score: 100.0, e: 0.08172\n","episode: 108/300, score: 116.0, e: 0.07994\n","episode: 109/300, score: 101.0, e: 0.07822\n","episode: 110/300, score: 104.0, e: 0.07653\n","episode: 111/300, score: 99.0, e: 0.07489\n","episode: 112/300, score: 112.0, e: 0.07329\n","episode: 113/300, score: 98.0, e: 0.07173\n","episode: 114/300, score: 87.0, e: 0.0702\n","episode: 115/300, score: 75.0, e: 0.06872\n","episode: 116/300, score: 84.0, e: 0.06727\n","episode: 117/300, score: 200.0, e: 0.06585\n","episode: 118/300, score: 79.0, e: 0.06447\n","episode: 119/300, score: 80.0, e: 0.06313\n","episode: 120/300, score: 82.0, e: 0.06182\n","episode: 121/300, score: 73.0, e: 0.06054\n","episode: 122/300, score: 81.0, e: 0.05929\n","episode: 123/300, score: 93.0, e: 0.05807\n","episode: 124/300, score: 71.0, e: 0.05689\n","episode: 125/300, score: 86.0, e: 0.05573\n","episode: 126/300, score: 70.0, e: 0.0546\n","episode: 127/300, score: 72.0, e: 0.0535\n","episode: 128/300, score: 69.0, e: 0.05242\n","episode: 129/300, score: 83.0, e: 0.05138\n","episode: 130/300, score: 119.0, e: 0.05035\n","episode: 131/300, score: 131.0, e: 0.04936\n","episode: 132/300, score: 139.0, e: 0.04839\n","episode: 133/300, score: 140.0, e: 0.04744\n","episode: 134/300, score: 151.0, e: 0.04651\n","episode: 135/300, score: 200.0, e: 0.04561\n","episode: 136/300, score: 200.0, e: 0.04473\n","episode: 137/300, score: 200.0, e: 0.04388\n","episode: 138/300, score: 200.0, e: 0.04304\n","episode: 139/300, score: 165.0, e: 0.04222\n","episode: 140/300, score: 200.0, e: 0.04143\n","episode: 141/300, score: 133.0, e: 0.04065\n","episode: 142/300, score: 200.0, e: 0.0399\n","episode: 143/300, score: 200.0, e: 0.03916\n","episode: 144/300, score: 200.0, e: 0.03844\n","episode: 145/300, score: 200.0, e: 0.03774\n","episode: 146/300, score: 200.0, e: 0.03705\n","episode: 147/300, score: 200.0, e: 0.03638\n","episode: 148/300, score: 193.0, e: 0.03573\n","episode: 149/300, score: 200.0, e: 0.0351\n","episode: 150/300, score: 100.0, e: 0.03448\n","episode: 151/300, score: 75.0, e: 0.03387\n","episode: 152/300, score: 200.0, e: 0.03328\n","episode: 153/300, score: 200.0, e: 0.03271\n","episode: 154/300, score: 200.0, e: 0.03215\n","episode: 155/300, score: 200.0, e: 0.0316\n","episode: 156/300, score: 200.0, e: 0.03107\n","episode: 157/300, score: 65.0, e: 0.03055\n","episode: 158/300, score: 200.0, e: 0.03004\n","episode: 159/300, score: 63.0, e: 0.02954\n","episode: 160/300, score: 66.0, e: 0.02906\n","episode: 161/300, score: 92.0, e: 0.02859\n","episode: 162/300, score: 200.0, e: 0.02813\n","episode: 163/300, score: 86.0, e: 0.02768\n","episode: 164/300, score: 200.0, e: 0.02725\n","episode: 165/300, score: 77.0, e: 0.02682\n","episode: 166/300, score: 200.0, e: 0.02641\n","episode: 167/300, score: 184.0, e: 0.026\n","episode: 168/300, score: 200.0, e: 0.02561\n","episode: 169/300, score: 200.0, e: 0.02522\n","episode: 170/300, score: 200.0, e: 0.02485\n","episode: 171/300, score: 78.0, e: 0.02448\n","episode: 172/300, score: 200.0, e: 0.02412\n","episode: 173/300, score: 200.0, e: 0.02377\n","episode: 174/300, score: 200.0, e: 0.02343\n","episode: 175/300, score: 200.0, e: 0.0231\n","episode: 176/300, score: 200.0, e: 0.02278\n","episode: 177/300, score: 198.0, e: 0.02246\n","episode: 178/300, score: 200.0, e: 0.02215\n","episode: 179/300, score: 80.0, e: 0.02185\n","episode: 180/300, score: 87.0, e: 0.02156\n","episode: 181/300, score: 200.0, e: 0.02128\n","episode: 182/300, score: 200.0, e: 0.021\n","episode: 183/300, score: 51.0, e: 0.02073\n","episode: 184/300, score: 200.0, e: 0.02046\n","episode: 185/300, score: 200.0, e: 0.0202\n","episode: 186/300, score: 200.0, e: 0.01995\n","episode: 187/300, score: 200.0, e: 0.01971\n","episode: 188/300, score: 200.0, e: 0.01947\n","episode: 189/300, score: 200.0, e: 0.01923\n","episode: 190/300, score: 200.0, e: 0.019\n","episode: 191/300, score: 200.0, e: 0.01878\n","episode: 192/300, score: 200.0, e: 0.01857\n","episode: 193/300, score: 200.0, e: 0.01835\n","episode: 194/300, score: 50.0, e: 0.01815\n","episode: 195/300, score: 58.0, e: 0.01795\n","episode: 196/300, score: 200.0, e: 0.01775\n","episode: 197/300, score: 93.0, e: 0.01756\n","episode: 198/300, score: 62.0, e: 0.01737\n","episode: 199/300, score: 41.0, e: 0.01719\n","episode: 200/300, score: 51.0, e: 0.01701\n","episode: 201/300, score: 200.0, e: 0.01684\n","episode: 202/300, score: 177.0, e: 0.01667\n","episode: 203/300, score: 37.0, e: 0.01651\n","episode: 204/300, score: 69.0, e: 0.01635\n","episode: 205/300, score: 30.0, e: 0.01619\n","episode: 206/300, score: 23.0, e: 0.01604\n","episode: 207/300, score: 200.0, e: 0.01589\n","episode: 208/300, score: 34.0, e: 0.01574\n","episode: 209/300, score: 36.0, e: 0.0156\n","episode: 210/300, score: 36.0, e: 0.01546\n","episode: 211/300, score: 25.0, e: 0.01533\n","episode: 212/300, score: 33.0, e: 0.0152\n","episode: 213/300, score: 28.0, e: 0.01507\n","episode: 214/300, score: 34.0, e: 0.01494\n","episode: 215/300, score: 169.0, e: 0.01482\n","episode: 216/300, score: 49.0, e: 0.0147\n","episode: 217/300, score: 48.0, e: 0.01458\n","episode: 218/300, score: 200.0, e: 0.01447\n","episode: 219/300, score: 62.0, e: 0.01436\n","episode: 220/300, score: 92.0, e: 0.01425\n","episode: 221/300, score: 83.0, e: 0.01415\n","episode: 222/300, score: 78.0, e: 0.01405\n","episode: 223/300, score: 172.0, e: 0.01395\n","episode: 224/300, score: 65.0, e: 0.01385\n","episode: 225/300, score: 56.0, e: 0.01375\n","episode: 226/300, score: 78.0, e: 0.01366\n","episode: 227/300, score: 148.0, e: 0.01357\n","episode: 228/300, score: 88.0, e: 0.01348\n","episode: 229/300, score: 114.0, e: 0.0134\n","episode: 230/300, score: 122.0, e: 0.01331\n","episode: 231/300, score: 88.0, e: 0.01323\n","episode: 232/300, score: 84.0, e: 0.01315\n","episode: 233/300, score: 50.0, e: 0.01307\n","episode: 234/300, score: 58.0, e: 0.013\n","episode: 235/300, score: 200.0, e: 0.01292\n","episode: 236/300, score: 109.0, e: 0.01285\n","episode: 237/300, score: 59.0, e: 0.01278\n","episode: 238/300, score: 62.0, e: 0.01271\n","episode: 239/300, score: 146.0, e: 0.01265\n","episode: 240/300, score: 77.0, e: 0.01258\n","episode: 241/300, score: 121.0, e: 0.01252\n","episode: 242/300, score: 200.0, e: 0.01245\n","episode: 243/300, score: 200.0, e: 0.01239\n","episode: 244/300, score: 200.0, e: 0.01233\n","episode: 245/300, score: 137.0, e: 0.01228\n","episode: 246/300, score: 123.0, e: 0.01222\n","episode: 247/300, score: 106.0, e: 0.01217\n","episode: 248/300, score: 200.0, e: 0.01211\n","episode: 249/300, score: 200.0, e: 0.01206\n","episode: 250/300, score: 200.0, e: 0.01201\n","episode: 251/300, score: 200.0, e: 0.01196\n","episode: 252/300, score: 129.0, e: 0.01191\n","episode: 253/300, score: 107.0, e: 0.01186\n","episode: 254/300, score: 118.0, e: 0.01182\n","episode: 255/300, score: 102.0, e: 0.01177\n","episode: 256/300, score: 79.0, e: 0.01173\n","episode: 257/300, score: 60.0, e: 0.01169\n","episode: 258/300, score: 60.0, e: 0.01164\n","episode: 259/300, score: 50.0, e: 0.0116\n","episode: 260/300, score: 42.0, e: 0.01156\n","episode: 261/300, score: 10.0, e: 0.01153\n","episode: 262/300, score: 13.0, e: 0.01149\n","episode: 263/300, score: 17.0, e: 0.01145\n","episode: 264/300, score: 13.0, e: 0.01142\n","episode: 265/300, score: 11.0, e: 0.01138\n","episode: 266/300, score: 9.0, e: 0.01135\n","episode: 267/300, score: 11.0, e: 0.01131\n","episode: 268/300, score: 10.0, e: 0.01128\n","episode: 269/300, score: 9.0, e: 0.01125\n","episode: 270/300, score: 10.0, e: 0.01122\n","episode: 271/300, score: 9.0, e: 0.01119\n","episode: 272/300, score: 10.0, e: 0.01116\n","episode: 273/300, score: 9.0, e: 0.01113\n","episode: 274/300, score: 11.0, e: 0.0111\n","episode: 275/300, score: 10.0, e: 0.01108\n","episode: 276/300, score: 10.0, e: 0.01105\n","episode: 277/300, score: 10.0, e: 0.01102\n","episode: 278/300, score: 9.0, e: 0.011\n","episode: 279/300, score: 9.0, e: 0.01097\n","episode: 280/300, score: 9.0, e: 0.01095\n","episode: 281/300, score: 9.0, e: 0.01093\n","episode: 282/300, score: 10.0, e: 0.0109\n","episode: 283/300, score: 9.0, e: 0.01088\n","episode: 284/300, score: 11.0, e: 0.01086\n","episode: 285/300, score: 52.0, e: 0.01084\n","episode: 286/300, score: 69.0, e: 0.01082\n","episode: 287/300, score: 200.0, e: 0.0108\n","episode: 288/300, score: 200.0, e: 0.01078\n","episode: 289/300, score: 200.0, e: 0.01076\n","episode: 290/300, score: 200.0, e: 0.01074\n","episode: 291/300, score: 200.0, e: 0.01072\n","episode: 292/300, score: 200.0, e: 0.0107\n","episode: 293/300, score: 200.0, e: 0.01069\n","episode: 294/300, score: 200.0, e: 0.01067\n","episode: 295/300, score: 143.0, e: 0.01065\n","episode: 296/300, score: 143.0, e: 0.01064\n","episode: 297/300, score: 132.0, e: 0.01062\n","episode: 298/300, score: 133.0, e: 0.01061\n","episode: 299/300, score: 132.0, e: 0.01059\n","episode: 300/300, score: 161.0, e: 0.01058\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"fhgY97l4fAL6"},"execution_count":null,"outputs":[]}]}