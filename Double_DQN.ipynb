{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOoDSXejIB1/zK/WVhHBa70"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s9vjPdW7G3ce","executionInfo":{"status":"ok","timestamp":1699785828724,"user_tz":-540,"elapsed":35921,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"6df1d759-a58c-4867-eb2c-60dc85b6fece"},"outputs":[{"output_type":"stream","name":"stderr","text":["에피소드 진행: 100%|██████████| 200/200 [00:35<00:00,  5.58it/s, episode_reward=12]"]},{"output_type":"stream","name":"stdout","text":["평균 점수: 56.105\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import random\n","from collections import deque\n","from tqdm import tqdm\n","\n","class Network(nn.Module):\n","    def __init__(self, state_size, action_size):\n","        super(Network, self).__init__()\n","        self.layer1 = nn.Linear(state_size, hidden_size)\n","        self.layer2 = nn.Linear(hidden_size, hidden_size)\n","        self.output = nn.Linear(hidden_size, action_size)\n","\n","    def forward(self, state):\n","        x = torch.relu(self.layer1(state))\n","        x = torch.relu(self.layer2(x))\n","        return self.output(x)\n","\n","class DQNAgent:\n","    def __init__(self, env, batch_size, target_update):\n","        self.env = env\n","        self.state_size = env.observation_space.shape[0]\n","        self.action_size = env.action_space.n\n","        self.batch_size = batch_size\n","        self.target_update = target_update\n","        self.gamma = 0.99\n","        self.model = Network(self.state_size, self.action_size)\n","        self.target_model = Network(self.state_size, self.action_size)\n","        self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)\n","        self.memory = deque(maxlen=2000)\n","        self.update_cnt = 0  # update_cnt 변수를 초기화\n","        self.update_target_model()\n","\n","    def get_action(self, state, epsilon):\n","        if random.random() < epsilon:\n","            return random.randrange(self.action_size)\n","        else:\n","            state = torch.FloatTensor(state).unsqueeze(0)\n","            q_value = self.model(state)\n","            return q_value.argmax().item()\n","\n","    def append_sample(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    def train_step(self):\n","        if len(self.memory) < self.batch_size:\n","            return\n","\n","        mini_batch = random.sample(self.memory, self.batch_size)\n","        states, actions, rewards, next_states, dones = zip(*mini_batch)\n","\n","        # 리스트를 먼저 numpy 배열로 변환한 후 torch tensor로 변환합니다\n","        states = np.array(states, dtype=np.float32)\n","        actions = np.array(actions, dtype=np.int64)\n","        rewards = np.array(rewards, dtype=np.float32)\n","        next_states = np.array(next_states, dtype=np.float32)\n","        dones = np.array(dones, dtype=np.float32)\n","\n","        states = torch.FloatTensor(states)\n","        actions = torch.LongTensor(actions)\n","        rewards = torch.FloatTensor(rewards)\n","        next_states = torch.FloatTensor(next_states)\n","        dones = torch.FloatTensor(dones)\n","\n","        self.update_cnt += 1  # train_step이 호출될 때마다 update_cnt를 증가\n","        if self.update_cnt % self.target_update == 0:\n","            self.update_target_model()\n","\n","        curr_q = self.model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n","        max_next_q = self.target_model(next_states).max(1)[0]\n","        expected_q = rewards + self.gamma * max_next_q * (1 - dones)\n","\n","        loss = nn.MSELoss()(curr_q, expected_q)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        self.optimizer.step()\n","\n","    def update_target_model(self):\n","        if self.update_cnt % self.target_update == 0:\n","            self.target_model.load_state_dict(self.model.state_dict())\n","\n","# 환경 설정 및 파라미터\n","env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","target_update = 100\n","hidden_size = 128\n","max_episodes = 200\n","batch_size = 64\n","epsilon = 1.0\n","max_epsilon = 1.0\n","min_epsilon = 0.01\n","decay_rate = 0.005\n","\n","# 에이전트 초기화\n","agent = DQNAgent(env, batch_size, target_update)\n","\n","# 훈련 루프\n","with tqdm(total=max_episodes, desc=\"에피소드 진행\") as pbar:\n","    for episode in range(max_episodes):\n","        state = env.reset()\n","        episode_reward = 0\n","        done = False\n","\n","        while not done:\n","            action = agent.get_action(state, epsilon)\n","            next_state, reward, done, _ = env.step(action)\n","            agent.append_sample(state, action, reward, next_state, done)\n","\n","            state = next_state\n","            episode_reward += reward\n","\n","            if len(agent.memory) >= agent.batch_size:\n","                agent.train_step()\n","                agent.update_target_model()\n","\n","        scores.append(episode_reward)\n","        epsilon = max(min_epsilon, epsilon * np.exp(-decay_rate))\n","        pbar.update(1)\n","        pbar.set_postfix({'episode_reward': episode_reward})\n","\n","print(f\"평균 점수: {sum(scores) / max_episodes}\")\n"]}]}