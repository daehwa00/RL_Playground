{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"UQoIIn4RFeDR","outputId":"571e5f2d-a54b-4088-f25e-952c8525dd12"},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-01-09 11:58:21--  https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4283 (4.2K) [text/plain]\n","Saving to: ‘segment_tree.py’\n","\n","segment_tree.py     100%[===================>]   4.18K  --.-KB/s    in 0s      \n","\n","2024-01-09 11:58:21 (33.7 MB/s) - ‘segment_tree.py’ saved [4283/4283]\n","\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n"]},{"name":"stdout","output_type":"stream","text":["cpu\n","Episode 1: 11.0\n","Episode 2: 27.0\n","Episode 3: 17.0\n","Episode 4: 12.0\n","Episode 5: 20.0\n","Episode 6: 20.0\n","Episode 7: 12.0\n","Episode 8: 11.0\n","Episode 9: 20.0\n","Episode 10: 11.0\n","Episode 11: 23.0\n","Episode 12: 16.0\n","Episode 13: 49.0\n","Episode 14: 32.0\n","Episode 15: 27.0\n","Episode 16: 15.0\n","Episode 17: 13.0\n","Episode 18: 11.0\n","Episode 19: 27.0\n","Episode 20: 59.0\n","Episode 21: 17.0\n","Episode 22: 25.0\n","Episode 23: 16.0\n","Episode 24: 19.0\n","Episode 25: 17.0\n","Episode 26: 28.0\n","Episode 27: 43.0\n","Episode 28: 30.0\n","Episode 29: 34.0\n","Episode 30: 43.0\n","Episode 31: 15.0\n","Episode 32: 125.0\n","Episode 33: 136.0\n","Episode 34: 40.0\n","Episode 35: 156.0\n","Episode 36: 161.0\n","Episode 37: 134.0\n","Episode 38: 200.0\n","Episode 39: 200.0\n","Episode 40: 200.0\n","Episode 41: 200.0\n","Episode 42: 165.0\n","Episode 43: 160.0\n","Episode 44: 200.0\n","Episode 45: 180.0\n","Episode 46: 170.0\n","Episode 47: 153.0\n","Episode 48: 156.0\n","Episode 49: 160.0\n","Episode 50: 139.0\n","Episode 51: 200.0\n","Episode 52: 156.0\n","Episode 53: 163.0\n","Episode 54: 200.0\n","Episode 55: 140.0\n","Episode 56: 154.0\n","Episode 57: 194.0\n","Episode 58: 162.0\n","Episode 59: 154.0\n","Episode 60: 132.0\n","Episode 61: 178.0\n","Episode 62: 200.0\n","Episode 63: 184.0\n","Episode 64: 114.0\n","Episode 65: 122.0\n","Episode 66: 134.0\n","Episode 67: 154.0\n","Episode 68: 128.0\n","Episode 69: 136.0\n","Episode 70: 138.0\n","Episode 71: 136.0\n","Episode 72: 135.0\n","Episode 73: 113.0\n","Episode 74: 125.0\n","Episode 75: 153.0\n","Episode 76: 134.0\n","Episode 77: 146.0\n","Episode 78: 153.0\n","Episode 79: 144.0\n","Episode 80: 148.0\n","Episode 81: 138.0\n","Episode 82: 170.0\n","Episode 83: 154.0\n","Episode 84: 163.0\n","Episode 85: 164.0\n","Episode 86: 200.0\n","Episode 87: 200.0\n","Episode 88: 128.0\n","Episode 89: 100.0\n","Episode 90: 137.0\n","Episode 91: 148.0\n","Episode 92: 156.0\n","Episode 93: 200.0\n","Episode 94: 172.0\n","Episode 95: 175.0\n","Episode 96: 200.0\n","Episode 97: 200.0\n","Episode 98: 200.0\n","Episode 99: 200.0\n","Episode 100: 200.0\n"]}],"source":["import sys\n","IN_COLAB = \"google.colab\" in sys.modules\n","\n","import math\n","import os\n","from collections import deque\n","import random\n","from typing import Deque, Dict, List, Tuple\n","\n","import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from IPython.display import clear_output\n","import matplotlib.pyplot as plt\n","from torch.nn.utils import clip_grad_norm_\n","\n","if torch.backends.cudnn.enabled:\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True\n","\n","seed = 777\n","torch.manual_seed(seed)\n","np.random.seed(seed)\n","random.seed(seed)\n","\n","class SumTree:\n","    data_pointer = 0\n","\n","    def __init__(self, capacity):\n","        self.capacity = capacity  # leaf node의 수 = capacity\n","        self.tree = np.zeros(2 * capacity - 1)  # 총 node의 수 -> 우선순위(priority)를 저장\n","        self.data = np.zeros(capacity, dtype=object)  # 경험(state, action, reward, next state, done flag로 이루어진 tuple)을 저장\n","        self.n_entries = 0\n","\n","    def add(self, priority, data):\n","        tree_index = self.data_pointer + self.capacity - 1\n","        self.data[self.data_pointer] = data # update data 프레임\n","        self.update(tree_index, priority) # leaf(priority) 업데이트\n","        self.data_pointer += 1  # pointer를 1 증가시킴\n","        if self.data_pointer >= self.capacity:  # capacity를 넘었다면 첫번째 index로 돌아감\n","            self.data_pointer = 0\n","        if self.n_entries < self.capacity:\n","            self.n_entries += 1\n","\n","    # leaf priority score 업데이트\n","    def _propagate(self, idx, change):\n","        parent = (idx - 1) // 2\n","        self.tree[parent] += change\n","        # parent가 0이면 중단. root node에 도달했기 때문\n","        if parent != 0:\n","            self._propagate(parent, change)\n","\n","    def update(self, tree_index, priority):\n","        change = priority - self.tree[tree_index]\n","        self.tree[tree_index] = priority\n","        self._propagate(tree_index, change)\n","\n","    # 이진 트리 구조를 사용하여 특정 조건을 만족하는 노드를 찾는 재귀 함수(recursive function)\n","    # 주어진 값 s에 대해 특정 조건을 만족하는 노드의 인덱스를 찾아라.\n","    def _retrieve(self, idx, s):\n","        left_child_index = 2 * idx + 1\n","        right_child_index = left_child_index + 1\n","        # 현재 노드가 leaf node(자식이 없는 node)인 경우를 검\n","        if left_child_index >= len(self.tree):\n","            return idx\n","        # s가 왼쪽 자식 노드에 저장된 값보다 작거나 같으면, 왼쪽 자식으로 재귀적으로 이동\n","        if s <= self.tree[left_child_index]:\n","            return self._retrieve(left_child_index, s)\n","        else:\n","            return self._retrieve(right_child_index, s - self.tree[left_child_index])\n","\n","    def get_leaf(self, s):\n","        leaf_index = self._retrieve(0, s)\n","        data_index = leaf_index - self.capacity + 1\n","        return (leaf_index, self.tree[leaf_index], self.data[data_index])\n","\n","    # 루트 노드를 반환\n","    def total_priority(self):\n","        return self.tree[0]\n","\n","class PrioritizedReplayBuffer(object):\n","    PER_e = 0.001 # 어떤 경험을 할 확률이 0이 되지 않도록 하는 hyperparameter\n","    PER_a = 0.6 # 우선순위가 높은 것과 무작위 샘플링 사이 절충을 하기 위한 hyperparameter\n","    PER_b = 0.4 # Importance Sampling. 1까지 증가\n","    PER_b_increment_per_sampling = 0.001\n","\n","    def __init__(self, capacity):\n","        self.tree = SumTree(capacity)\n","        self.capacity = capacity\n","\n","    # 최대 우선 순위 검색\n","    def _getPriority(self, error):\n","        return (error + self.PER_e) ** self.PER_a\n","\n","    def store(self, error, sample):\n","        max_priority = self._getPriority(error)\n","        self.tree.add(max_priority, sample)\n","\n","    def sample(self, n):\n","        minibatch = []\n","        idxs = []\n","        priority_segment = self.tree.total_priority() / n\n","        priorities = []\n","        self.PER_b = np.min([1., self.PER_b + self.PER_b_increment_per_sampling])\n","\n","        for i in range(n):\n","            a = priority_segment * i\n","            b = priority_segment * (i + 1)\n","            value = np.random.uniform(a, b)\n","            (idx, p, data) = self.tree.get_leaf(value)\n","            priorities.append(p)\n","            minibatch.append(data)\n","            idxs.append(idx)\n","\n","        sampling_probabilities = priorities / self.tree.total_priority()\n","        is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.PER_b)\n","        is_weight /= is_weight.max()\n","\n","        return minibatch, idxs, is_weight\n","\n","    def batch_update(self, idx, error):\n","        p = self._getPriority(error)\n","        self.tree.update(idx, p)\n","\n","def append_sample(self, state, action, reward, next_state, done):\n","    # PyTorch 텐서로 변환\n","    state = torch.FloatTensor(state).unsqueeze(0)\n","    next_state = torch.FloatTensor(next_state).unsqueeze(0)\n","    action = torch.LongTensor([action])\n","    reward = torch.FloatTensor([reward])\n","    done = torch.FloatTensor([done])\n","\n","    # Q 값 계산\n","    with torch.no_grad():\n","        main_next_q = self.dqn(next_state)\n","        next_action = main_next_q.max(1)[1].view(1, 1)\n","        target_next_q = self.dqn_target(next_state)\n","        target_value = target_next_q.gather(1, next_action).item()\n","\n","    target_value = reward + (self.gamma * target_value * (1 - done))\n","\n","    # 현재 상태에 대한 Q 값\n","    main_q = self.dqn(state).gather(1, action.unsqueeze(1)).item()\n","\n","    # TD 오차 계산\n","    td_error = abs(target_value - main_q)\n","\n","    # 메모리에 경험 저장\n","    self.MEMORY.store(td_error, (state, action, reward, next_state, done))\n","\n","\n","class Network(nn.Module):\n","    def __init__(self, state_size: int, action_size: int,\n","    ):\n","        \"\"\"Initialization.\"\"\"\n","        super(Network, self).__init__()\n","\n","        self.layers = nn.Sequential(\n","            nn.Linear(state_size, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, action_size)\n","        )\n","\n","    def forward(self, x: torch.Tensor) -> torch.Tensor:\n","        \"\"\"Forward method implementation.\"\"\"\n","        return self.layers(x)\n","\n","class DQNAgent:\n","    \"\"\"DQN Agent interacting with environment.\n","\n","    Attribute:\n","        env (gym.Env): openAI Gym environment\n","        memory (PrioritizedReplayBuffer): replay memory to store transitions\n","        batch_size (int): batch size for sampling\n","        epsilon (float): parameter for epsilon greedy policy\n","        epsilon_decay (float): step size to decrease epsilon\n","        max_epsilon (float): max value of epsilon\n","        min_epsilon (float): min value of epsilon\n","        target_update (int): period for target model's hard update\n","        gamma (float): discount factor\n","        dqn (Network): model to train and select actions\n","        dqn_target (Network): target model to update\n","        optimizer (torch.optim): optimizer for training dqn\n","        transition (list): transition information including\n","                           state, action, reward, next_state, done\n","        beta (float): determines how much importance sampling is used\n","        prior_eps (float): guarantees every transition can be sampled\n","        use_n_step (bool): whether to use n_step memory\n","        n_step (int): step number to calculate n-step td error\n","        memory_n (ReplayBuffer): n-step replay buffer\n","    \"\"\"\n","\n","    def __init__(\n","        self,\n","        env: gym.Env,\n","        memory_size: int,\n","        batch_size: int,\n","        target_update: int,\n","        epsilon_decay: float,\n","        max_epsilon: float = 1.0,\n","        min_epsilon: float = 0.1,\n","        gamma: float = 0.99,\n","        # PER parameters\n","        alpha: float = 0.2,\n","        beta: float = 0.6,\n","        prior_eps: float = 1e-6,\n","        # N-step Learning\n","        n_step: int = 3,\n","    ):\n","        \"\"\"\n","        Initialization.\n","\n","        Args:\n","            env (gym.Env): openAI Gym environment\n","            memory_size (int): length of memory\n","            batch_size (int): batch size for sampling\n","            target_update (int): period for target model's hard update\n","            epsilon_decay (float): step size to decrease epsilon\n","            lr (float): learning rate\n","            max_epsilon (float): max value of epsilon\n","            min_epsilon (float): min value of epsilon\n","            gamma (float): discount factor\n","            alpha (float)    : determines how much prioritization is used\n","            beta (float)     : determines how much importance sampling is used\n","            prior_eps (float): guarantees every transition can be sampled\n","            n_step (int): step number to calculate n-step td error\n","        \"\"\"\n","        self.env = env\n","        # network parameters\n","        self.state_size = self.env.observation_space.shape[0]\n","        self.action_size = self.env.action_space.n\n","\n","        # hyperparameters\n","        self.batch_size = batch_size\n","        self.epsilon = max_epsilon\n","        self.epsilon_decay = epsilon_decay\n","        self.max_epsilon = max_epsilon\n","        self.min_epsilon = min_epsilon\n","        self.target_update = target_update\n","        self.gamma = gamma\n","\n","\n","        # device: cpu / gpu\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        print(self.device)\n","\n","        # PER\n","        # memory for 1-step Learning\n","        self.beta = beta\n","        self.prior_eps = prior_eps\n","        self.memory = PrioritizedReplayBuffer(\n","            self.state_size, memory_size, batch_size, alpha\n","        )\n","\n","        # memory for N-step Learning\n","        self.use_n_step = True if n_step > 1 else False\n","        if self.use_n_step:\n","            self.n_step = n_step\n","            self.memory_n = ReplayBuffer(\n","                self.state_size,\n","                memory_size,\n","                batch_size,\n","                n_step=n_step,\n","                gamma=gamma\n","            )\n","\n","        # networks: dqn, dqn_target\n","        self.dqn = Network(self.state_size, self.action_size\n","                          ).to(self.device)\n","        self.dqn_target = Network(self.state_size, self.action_size\n","                          ).to(self.device)\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n","        self.dqn_target.eval()\n","\n","        # optimizer\n","        self.optimizer = optim.Adam(self.dqn.parameters())\n","\n","        # transition to store in memory\n","        self.transition = list()\n","\n","        # mode: train / test\n","        self.is_test = False\n","\n","    def get_action(self, state: np.ndarray) -> np.ndarray:\n","        \"\"\"Select an action from the input state.\"\"\"\n","        # epsilon greedy policy\n","        if self.epsilon > np.random.random():\n","            selected_action = self.env.action_space.sample()\n","        else:\n","            selected_action = self.dqn(\n","                torch.FloatTensor(state).to(self.device)\n","            ).argmax()\n","            selected_action = selected_action.detach().cpu().numpy()\n","\n","        if not self.is_test:\n","            self.transition = [state, selected_action]\n","\n","        return selected_action\n","\n","    def _compute_dqn_loss(\n","        self, samples: Dict[str, np.ndarray], gamma: float\n","    ) -> torch.Tensor:\n","        \"\"\"Return dqn loss.\"\"\"\n","        device     = self.device  # for shortening the following lines\n","\n","        state      = torch.FloatTensor(samples[\"obs\"]).to(device)\n","        action     = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n","        reward     = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n","        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n","        done       = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n","\n","        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n","        #       = r                       otherwise\n","        curr_Qs = self.dqn(state).gather(1, action)\n","        next_Q_targs = self.dqn_target(\n","            next_state\n","        ).max(dim=1, keepdim=True)[0].detach()\n","        mask = 1 - done\n","        target_value = (reward + self.gamma * next_Q_targs * mask).to(self.device)\n","\n","        # calculate element-wise dqn loss\n","        elementwise_loss = F.smooth_l1_loss(curr_Qs, target_value, reduction=\"none\")\n","\n","        return elementwise_loss\n","\n","    def train_step(self) -> torch.Tensor:\n","        \"\"\"Update the model by gradient descent.\"\"\"\n","        # PER needs beta to calculate weights\n","\n","        samples = self.memory.sample_batch(self.beta)\n","        weights = torch.FloatTensor(samples[\"weights\"].reshape(-1, 1)).to(self.device)\n","        indices = samples[\"indices\"]\n","\n","        # 1-step Learning loss\n","        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)\n","\n","        # PER: importance sampling before average\n","        loss = torch.mean(elementwise_loss * weights)\n","\n","        # N-step Learning loss\n","        # we are gonna combine 1-step loss and n-step loss so as to\n","        # prevent high-variance. The original rainbow employs n-step loss only.\n","        if self.use_n_step:\n","            gamma = self.gamma ** self.n_step\n","            samples = self.memory_n.sample_batch_from_idxs(indices)\n","            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)\n","            elementwise_loss += elementwise_loss_n_loss\n","\n","            # PER: importance sampling before average\n","            loss = torch.mean(elementwise_loss * weights)\n","\n","        self.optimizer.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(self.dqn.parameters(), 10.0)\n","        self.optimizer.step()\n","\n","        # PER: update priorities\n","        loss_for_prior = elementwise_loss.detach().cpu().numpy()\n","        new_priorities = loss_for_prior + self.prior_eps\n","        self.memory.update_priorities(indices, new_priorities)\n","\n","        return loss.item()\n","\n","    def _target_hard_update(self):\n","        \"\"\"Hard update: target <- local.\"\"\"\n","        self.dqn_target.load_state_dict(self.dqn.state_dict())\n","\n","# environment\n","env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","\n","\n","\n","memory_size = 2000\n","target_update = 100\n","epsilon_decay = 1 / 2000\n","initial_random_steps = 5000\n","\n","max_episodes = 100\n","batch_size = 32\n","\n","\n","agent = DQNAgent(\n","    env,\n","    memory_size,\n","    batch_size,\n","    target_update,\n","    epsilon_decay,\n",")\n","\n","if __name__ == \"__main__\":\n","\n","    \"\"\"Train the agent.\"\"\"\n","    agent.is_test = False\n","\n","    update_cnt    = 0\n","    epsilons      = []\n","    losses        = []\n","    scores        = []\n","    frame_idx = 0\n","    num_frames= 100000\n","\n","    # EACH EPISODE\n","    for episode in range(max_episodes):\n","        ## Reset environment and get first new observation\n","        state = agent.env.reset()\n","        episode_reward = 0\n","        done = False  # has the enviroment finished?\n","\n","        while not done:\n","            '''\n","            Get Action\n","            '''\n","            action = agent.get_action(state)\n","\n","            '''\n","            Execute Action and Observe\n","            '''\n","            next_state, reward, done, _ = agent.env.step(action)\n","\n","            '''\n","            Store Transitions\n","            '''\n","            agent.transition += [reward, next_state, done]\n","\n","            # N-step transition\n","            if agent.use_n_step:\n","                one_step_transition = agent.memory_n.store(*agent.transition)\n","            # 1-step transition\n","            else:\n","                one_step_transition = agent.transition\n","\n","            # add a single step transition\n","            if one_step_transition:\n","                agent.memory.store(*one_step_transition)\n","\n","            state = next_state\n","            episode_reward += reward\n","\n","            frame_idx += 1\n","\n","            # PER: increase beta\n","            fraction = min(frame_idx / num_frames, 1.0)\n","            agent.beta = agent.beta + fraction * (1.0 - agent.beta)\n","\n","            # if episode ends\n","            if done:\n","                state = agent.env.reset()\n","                scores.append(episode_reward)\n","                print(\"Episode \" + str(episode+1) + \": \" + str(episode_reward))\n","\n","            # if training is ready\n","            if (len(agent.memory) >= agent.batch_size):\n","                loss = agent.train_step()\n","                losses.append(loss)\n","                update_cnt += 1\n","\n","                # linearly decrease epsilon\n","                agent.epsilon = max(\n","                    agent.min_epsilon, agent.epsilon - (\n","                        agent.max_epsilon - agent.min_epsilon\n","                    ) * agent.epsilon_decay\n","                )\n","                epsilons.append(agent.epsilon)\n","\n","                # if hard update is needed\n","                if update_cnt % agent.target_update == 0:\n","                    agent._target_hard_update()\n","\n"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvFSQwFvB9P2GnJf0ByHpy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}