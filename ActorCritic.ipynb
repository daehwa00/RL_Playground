{"cells":[{"cell_type":"code","source":["import gym\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.distributions as distributions\n","from tqdm import tqdm\n","\n","# Actor Model\n","class Actor(nn.Module):\n","    def __init__(self, state_size, action_size, hidden_size):\n","        super(Actor, self).__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(state_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, action_size),\n","            nn.Softmax(dim=-1)\n","        )\n","\n","    def forward(self, state):\n","        return self.network(state)\n","\n","\n","# Critic Model\n","class Critic(nn.Module):\n","    def __init__(self, state_size, hidden_size):\n","        super(Critic, self).__init__()\n","        self.network = nn.Sequential(\n","            nn.Linear(state_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, hidden_size),\n","            nn.ReLU(),\n","            nn.Linear(hidden_size, 1)\n","        )\n","\n","    def forward(self, state):\n","        return self.network(state)\n","\n","\n","# A2C Agent\n","class A2CAgent:\n","    def __init__(self, env, state_size, action_size, hidden_size):\n","        self.env = env\n","        self.actor = Actor(state_size, action_size, hidden_size)\n","        self.critic = Critic(state_size, hidden_size)\n","        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=7e-3)\n","        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=7e-3)\n","        self.gamma = 0.99\n","\n","    def get_action(self, state):\n","        state = torch.tensor([state], dtype=torch.float)\n","        probs = self.actor(state)\n","        dist = distributions.Categorical(probs)\n","        action = dist.sample()\n","        return action.item()\n","\n","    def train_step(self, state, action, reward, next_state, done):\n","        state = torch.tensor([state], dtype=torch.float)\n","        next_state = torch.tensor([next_state], dtype=torch.float)\n","        action = torch.tensor([action], dtype=torch.int)\n","        reward = torch.tensor([reward], dtype=torch.float)\n","        done = torch.tensor([done], dtype=torch.float)\n","\n","        # Calculate loss\n","        curr_Q = self.critic(state)\n","        next_Q = self.critic(next_state)\n","        expected_Q = reward + self.gamma * next_Q * (1 - done)\n","        TD = expected_Q - curr_Q\n","\n","        critic_loss = nn.MSELoss()(curr_Q, expected_Q.detach())\n","        self.critic_optimizer.zero_grad()\n","        critic_loss.backward()\n","        self.critic_optimizer.step()\n","\n","        probs = self.actor(state)\n","        dist = distributions.Categorical(probs)\n","        log_prob = dist.log_prob(action)\n","        actor_loss = -(log_prob * TD.detach()).mean()\n","        self.actor_optimizer.zero_grad()\n","        actor_loss.backward()\n","        self.actor_optimizer.step()\n","\n","        return actor_loss.item(), critic_loss.item()\n","\n","\n","# Training Loop\n","env_name = \"CartPole-v0\"\n","env = gym.make(env_name)\n","env.seed(2000)\n","\n","state_size = env.observation_space.shape[0]\n","action_size = env.action_space.n\n","hidden_size = 32\n","max_episodes = 300\n","\n","agent = A2CAgent(env, state_size, action_size, hidden_size)\n","\n","# Create a tqdm iterator object\n","progress_bar = tqdm(range(max_episodes), desc=\"Training Progress\")\n","\n","for episode in progress_bar:\n","    state = env.reset()\n","    episode_reward = 0\n","    done = False\n","\n","    while not done:\n","        action = agent.get_action(state)\n","        next_state, reward, done, _ = env.step(action)\n","        aloss, closs = agent.train_step(state, action, reward, next_state, done)\n","        state = next_state\n","        episode_reward += reward\n","\n","        if done:\n","            progress_bar.set_postfix({'Episode': episode + 1, 'Reward': episode_reward})\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k48qaUzGw6V_","executionInfo":{"status":"ok","timestamp":1706945529596,"user_tz":-540,"elapsed":22572,"user":{"displayName":"고대화","userId":"04859921805535355794"}},"outputId":"8a008a7e-be7d-4e8e-cbe5-b7f9c7a26c0f"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","  logger.warn(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  deprecation(\n","/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n","  deprecation(\n","Training Progress: 100%|██████████| 300/300 [00:22<00:00, 13.37it/s, Episode=300, Reward=9]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EfcmDndRw640"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOQ5oQYiUlmCRrpI998WVNk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}